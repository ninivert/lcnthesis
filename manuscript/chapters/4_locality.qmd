::: {.content-hidden}
{{< include ../macros.tex >}}

```{python}
import matplotlib as mpl, matplotlib.pyplot as plt, matplotlib.colors as mplcolors, matplotlib.patheffects as pe, matplotlib.ticker as ticker
import numpy as np
import sys
sys.path.append('../../notebooks')  # import neurodyn
from neurodyn import *
from matplotlib_tufte import *
setup()

mpl.use('svg')

def mktrans(fig, ax):
	"""Makes figure and axes transparent"""
	fig.patch.set_facecolor('#FFFFFF00')
	ax.patch.set_facecolor('#FFFFFF00')
```
:::

# Locality and numerical convergence {#sec-locality}

The previous chapter gave a detailed description of the mappings $S : [0,1]^2 \mapsto [0,1]$, as well as a few numerical considerations. We found that mappings that yield equivalent dynamics crucially seem to have in common the property of *locality* (from 1D to 2D), even if the image seems to have a fractal structure. Intuitively, we saw that the locality seems to emerge from the mapping having for the most part small discontinuities.

In this chapter, we will further explore these results, and attempt to formulate a measure of this "locality". As a preamble, we show that bijectivity and measurability are sufficient conditions for  the 2D and 1D neural field dynamics to be equivalent. Then, we apply our measure of "locality" to the special cases of the Column and Z-mappings, and show how this notion can discriminate between the two mappings. Finally, we show that our measure of locality is sufficient to ensure that the numerical estimation of the integral converges to the analytical integral; therefore, such embeddings can be simulated numerically.

## Analytical equivalence of the dynamics when $S$ is measurable and bijective

<!-- In the previous chapter, we introduced Cantor and Peano functions, and argued that if have to choose between the two, then the non-injective Peano functions are preferable, because, by Netto's theorem, they are the only ones that can be space-filling and reach all the square populations. -->
<!-- Let us make the assumption that $S$ is a bijective function. However, to not break Netto's theorem, we do not make any assumptions regarding its continuity.

Nicole: let's not mention Netto's theorem here. It applies to S^{-1}, and it becomes difficult to reason about when we talk about S. I don't think the proof works strictly in our case, because the Z-mapping is not a bijection, but let us assume uwuwuwu

Nicole again: our Z-mapping is not defined in the same way as the Lebesgue curve, because we use indicator functions to take bit representations. We are stepwise functions. Our Z-mapping is a bijection, and not continuous.
-->

Let us assume that the mapping $S$ is bijective and measurable. This is not a strong assumption, since the limit of pointwise convergent measurable functions is also measurable. In our case, we have written the finite-$n$ expansions $S^n$, which, by being sums of piecewise constant functions, are also measurable. The measurability of $S$ allows us to write $\lambda \circ S^{-1}$, the pushforward measure (or image measure) of $\lambda$ under the mapping $S$.

With these two assumptions, let us prove that by defining the initial condition and connectivity
$$
\begin{aligned}
\tilde h(\alpha, t=0) &= h_U(S^{-1}(\alpha), t=0)\\
\tilde w(\alpha, \beta) &= w_U(S^{-1}(\alpha), S^{-1}(\beta)),
\end{aligned}
$$

then the solution $h_U(\vec v, t)$ of the neural field equation in $[0,1]^2$ uniquely defines the solution of the neural field equation in $[0,1]$ by writing

$$
\tilde h(\alpha, t) = h_U(\cdot, t) \circ S^{-1}(\alpha).
$$ {#eq-analytical-equivalence-tildeh}

We start by recalling the expression of the one-dimensional neural field, then substitute @eq-analytical-equivalence-tildeh and $\vec{v} = S^{-1}(\alpha),\, \vec{u} = S^{-1}(\beta)$.

$$
\begin{aligned}
\partial_t \tilde h(\alpha, t) &= -\tilde h(\alpha, t) + \int_{[0,1]} \tilde w(\alpha, \beta) \phi(\tilde h(\beta, t)) \left[\lambda \circ S^{-1}\right](\d \beta) \\
\iff&\\
\partial_t h_U(S^{-1}(\alpha), t) &= 
\begin{aligned}[t]
	&-h_U(S^{-1}(\alpha), t)\\&+ \int_{[0,1]} w(S^{-1}(\alpha), S^{-1}(\beta)) \phi(h_U(S^{-1}(\beta), t)) \left[\lambda \circ S^{-1}\right](\d \beta)
\end{aligned}\\
&= \begin{aligned}[t]
	&-h_U(\vec v, t)\\&+ \int_{[0,1]} w(\vec v, S^{-1}(\beta)) \phi(h_U(S^{-1}(\beta), t)) \left[\lambda \circ S^{-1}\right](\d \beta)
\end{aligned}\\
&= -h_U(\vec v, t) + \int_{[0,1]^2} w(\vec v, \vec u) \phi(h_U(\vec u, t)) \lambda(\d \vec u)
\end{aligned}
$$

In the last line, we used a basic property of the pushfoward measure (see @Bogachev2007, Theorem 3.6.1, p. 190). The Lebesgue measure $\lambda : [0,1]^2 \mapsto \R^+$ simply plays the role of a uniform distribution on $[0,1]^2$, and we simply write

$$
\partial_t h_U(\vec v, t) = -h_U(\vec v, t) + \int_{[0,1]^2} w(\vec v, \vec u) \phi(h_U(\vec u, t)) \d \vec u.
$$

Therefore, if $h_U(\vec v, t)$ solves the neural field equation in $[0,1]^2$, then $\tilde h(\alpha, t) = (h_U(\cdot, t) \circ S^{-1})(\alpha)$ solves the equivalent neural field equation in $[0,1]$. The same reasoning applies in reverse: if $\tilde h(\alpha, t)$ solves the neural field equation in $[0,1]$, then $h_U(\vec v, t) = (\tilde h(\cdot, t) \circ S)(\vec v)$ solves the neural field in $[0,1]^2$.

We do not address the question of well-posedness of the neural field equations in this thesis; for proofs of the well-posedness of neural field equations, we refer the reader to [@faugeras2008absolute;@faugeras2009persistent;@veltz2009localglobal].

## Locality

<!-- ### The total variation attempt

The previous chapter gave the intuition that local mappings map populations close in 1D to populations close in 2D. To verify this, we might try to compare the distance between all segment populations and the population at $\alpha=0$, to the distance of all square populations to the population at $S^{-1}(\alpha=0)$. This is done in @fig-distance-2d-1d, where the 2D distance is taken to be the $\ell^2$ norm. If the distances matched perfectly, then we would see a line with slope $\sqrt 2$ in the graph. 

![(TODO random, column and zcurve only, rename neuron to population) Distance between the first population and all other populations, in 1D and 2D. The dotted line has a slope $\sqrt 2$.](figures/distance_2d_1d.png){#fig-distance-2d-1d}

For control, we also demonstrate that the random mapping has no locality: there is no correlation between the positions in 1D and the positions in 2D.
Both Column and Z-mappings show a clear trend, the distances in 2D are correlated to the distances in 1D. The notable difference is that distances for the Column mapping seem to "oscillate" wildly, while the Z-mapping seems much more regular and matches the straight line more closely.

This analysis motivates the following hypothesis: the property that sets the mappings apart, is that of total variation. The idea is to "project and sum up" all of the variations defined by the curves as they trace a path in the $[0,1]^2$ embedding. As $n\to\infty$, we expect to see that the total variation of the Z-mapping grows much more slowly than that of the Column mapping.

Denoting $\alpha_i,\,i \in \{1, \cdots, 4^n\}$ the positions of the grid in the $[0,1]$ embedding, we define the total variation as the projection of all the corresponding 2D jumps onto the axes:[^tv]

$$
\begin{aligned}
\mathrm{TV}(S^{-1}) &= \sum_{i=2}^{4^n} \norm{S^{-1}(\alpha_i) - S^{-1}(\alpha_{i-1})}_1 \\
&= \sum_{i=2}^{4^n} |S_1^{-1}(\alpha_i) - S_1^{-1}(\alpha_{i-1})| + |S_2^{-1}(\alpha_i) - S_2^{-1}(\alpha_{i-1})|.
\end{aligned}
$$ {#eq-tv}

[^tv]:
  This definition is slightly different from other definitions of total variation, which involve a supremum over all partitions $P = \{0 \leq \alpha_1 < \cdots < \alpha_{N_p} \leq 1\}$.
  <!-- The total variation is then defined as
  $$
  \mathrm{TV}(S^{-1}) = \sup_{P} \sum_{i=1}^{N_P} \norm{S^{-1}(\alpha_i) - S^{-1}(\alpha_{i-1})}_1.
  $$
  We don't use the more complex definition, because our version of total variation is just used for illustration purposes. Additionally, we make use of the $\ell^1$ norm because it lends itself to the nice geometric intuition of "projecting the jumps" on the axes. This choice of the norm is irrelevant, however, because analytically all norms are equivalent up to a constant.

It is easy to see that $2\times 4^n$ is an upper bound of $\mathrm{TV}(S^{-1})$, since the image of $S^{-1}$ is bounded in $[0,1]^2$. In @fig-tv, we compute the total variation for different $n$ corresponding to different mappings.

![(TODO $2\times 4^n$) Total variation for different mappings, as a function of $n$](figures/tv.png){#fig-tv}

Disappointingly, all mappings seem to exhibit the same growth of total variation. It seems like our notion of total variation is not adequate to help discriminate between the mappings. We need to rethink our intuition.

In some sense, having the constraint that the mapping traces a path in the bounded space $[0,1]^2$ that can go through each point of the grid only once leaves little flexibility. For fixed $n$, there are only finitely many "large jumps" the mapping can make, and the large number of "small jumps" still add up, so that the total variation still grows at approximately the same rate. -->

### Locality as vanishing average binned variation {#sec-Vn}

The previous chapter gave the intuition that local mappings map populations close in 1D to populations close in 2D. To verify this, we might try to compare the distance between all segment populations and the population at $\alpha=0$, to the distance of all square populations to the population at $S^{-1}(\alpha=0)$. This is done in @fig-distance-2d-1d, where the 2D distance is taken to be the $\ell^2$ norm. If the distances matched perfectly, then we would see a line with slope $\sqrt 2$ in the graph. 

```{python}
#| label: fig-distance-2d-1d
#| fig-cap: Distance between the first population and all other populations, in 1D and 2D. The dotted line has a slope $\sqrt 2$. This figure is generated using $n=6$.
#| echo: false

mapping_clss = {
	'Random mapping': RandomMapping,
	'Column mapping': ColumnMapping,
	'Z-mapping': ZMapping,
}

fig, axes = plt.subplots(ncols=len(mapping_clss), figsize=(len(mapping_clss)*3, 3))

isfirst = True
for ax, (name, mapping_cls) in zip(axes, mapping_clss.items()):
	F = mapping_cls.new_nrec(6).inverse_samples()
	a = np.arange(len(F)) / len(F)
	a_ = np.linspace(-0.02, 1.02, 3)
	ax.plot(a, np.linalg.norm(F[0] - F, axis=1), linewidth=0.5)
	ax.plot(a_, a_*np.sqrt(2), '--', clip_on=False) 
	ax.set_xlabel('Distance in 1D')
	if isfirst:
		isfirst = False
		ax.set_ylabel('Distance in 2D')
	else:
		ax.yaxis.set_visible(False)
		ax.spines.left.set_visible(False)
	ax.label_outer()
	ax.set_title(name)
	ax.set_xlim((0,1))
	ax.set_ylim((0, 1.6))
	mktrans(fig, ax)
	breathe(ax)
	
# %matplotlib inline
display(fig)
```

For control, we also demonstrate that the random mapping has no locality: there is no correlation between the positions in 1D and the positions in 2D.
Both Column and Z-mappings show a clear trend, the distances in 2D are correlated to the distances in 1D. The notable difference is that distances for the Column mapping seem to "oscillate" wildly, while the Z-mapping seems much more regular and matches the straight line more closely.

With this view, we understand that the coarse-graining performed in @sec-coarse-graining makes use of that fact that locality helps by bounding the variation inside of each averaged bin. Therefore, we would like for mathematically formulate that on average, the variation inside of each bin before coarse-graining is small. @fig-sketch-binned-variation gives an intuition of this. Even though the $[0,1]$ embedding might be discontinuous in places, on average nearby neurons should exhibit the same activity levels.

![Intuition behind the average binned variation. To simplify the visualization, activity levels are used in lieu of the 2D positions. 4 bins are each shown with a different colour. The horizontal line represents the average activity in each bin. From top to bottom: Random, Column, Z-mapping.](figures/fig-variation.svg){width=600px #fig-sketch-binned-variation}

Given $n$, let us define the average binned variation as the maximum difference of the 2D positions associated to segment populations in the same bin of size $2^{-n}$:

$$
V_n(S^{-1}) = \frac{1}{2^n}\sum_{i=1}^{2^n} \sup_{\alpha, \alpha^\prime \in \left[\tfrac{i-1}{2^n}, \tfrac{i}{2^n}\right]^2} \norm{S^{-1}(\alpha^\prime) - S^{-1}(\alpha)}_1
$$ {#eq-Vn}

The definition of locality can essentially be seen as a weakened version of continuity, or "continuity on average". A continuous function $f$ evidently verifies $V_n(f) \xrightarrow{n \to \infty} 0$, since small neighbourhoods are mapped to small neighbourhoods. The reverse is not true, since for instance the metric $V_n$ vanishes when applied to a step function, which is only piecewise continuous.

<!--
variation inside the bins, strong version. basically, we take the max over all binning methods that have bins smaller than $\delta$. Let $P_\delta$ be a permutation of $[0,1]$ with $n_{P_\delta}$ elements $\{\alpha_1 \leq \alpha_2 \leq \cdots \alpha_{n_{P_\delta}}\}$, where $\alpha_{i}-\alpha_{i-1} \leq \delta$, and $\mathcal{P}_\delta$ the set of such permutations.

$$
\begin{aligned}
V_\delta(S^{-1}) &= \sup_{P_\delta \in \mathcal{P}_\delta} \delta \sum_{i=1}^{n_{P_\delta}} \sup_{\alpha, \alpha^\prime \in [\alpha_{i-1}, \alpha_{i}]^2} \norm{S^{-1}(\alpha^\prime) - S^{-1}(\alpha)}_1 \\
V &= \limsup_{\delta \to 0} V_\delta
\end{aligned}
$$

evidently,

$$
0 \leq V_n \leq V_\delta
$$
-->

<!-- TODONE : I have removed the definition involving the sup over all permutations, because we don't end up using it. Still mention for completeness ? 

Valentin: For the master thesis, it is ok to not mention it.
-->

<!-- TODONE : in the definition of $V$, we use the converged version of $S$, instead of a finite truncation $S^n$. we need to convince people this is not a big mistake, since $S^n \to S$, and we can take both limits (binning and $S^n$) "at the same time". This is adressed by a footnote in @sec-mappings-binary -->

Before we move to analytical derivations applying the average variation to mappings, we can first check numerically if it gives the expected results; that is $V_n$ vanishes when $n\to\infty$ for local mappings, whereas $V_n$ should be lower bounded by a constant for non-local mappings. @fig-vnum shows this is indeed the case, rejoice!

```{python}
#| label: fig-vnum
#| fig-cap: (Numerator of the) average binned variation as a function of $n$. The Z-mapping has a growth slower than the upper bound $2\times 2^n$. The Column mapping grows at the same rate as the upper bound, and the Random mapping saturates it.
#| echo: false

def V(F: np.ndarray) -> np.ndarray:
	nrec = int(np.emath.logn(4, len(F)))
	F_ = F.reshape((2**nrec, 2**nrec, 2))
	# F_ = np.roll(F, 2**nrec).reshape((2**nrec, 2**nrec, 2))  # offset the bins halfway to test
	return (F_.max(axis=1) - F_.min(axis=1)).sum(axis=0)

mapping_clss = {'Random': RandomMapping, 'Column': ColumnMapping, 'Z': ZMapping}
mapping_Vs = { key: [] for key in mapping_clss }
nrecs = np.array([1,2,3,4,5,6,7,8,9])

for name, mapping_cls in mapping_clss.items():
	for nrec in nrecs:
		F = mapping_cls.new_nrec(nrec).inverse_samples(centered=False)
		mapping_Vs[name].append(V(F))

import itertools
fig, ax = plt.subplots(figsize=(4, 3))
marker = itertools.cycle(('s', 'd', '^', 'o', 'P'))
colors = itertools.cycle(('C2', 'C1', 'C0'))
ax.plot(nrecs, 2**(nrecs+1), color='k', linestyle='--', label='upper bound', clip_on=False, zorder=10)
for name, Vs in mapping_Vs.items():
	ax.plot(nrecs, np.array(Vs).sum(axis=-1), marker=next(marker), color=next(colors), label=name, clip_on=False)
ax.set_xlabel('$n$')
ax.set_yscale('log', base=2)
ax.set_autoscaley_on(False)
# ax.grid(True, clip_on=False)
# ax.legend(loc='upper left', bbox_to_anchor=(1,1))
ax.legend(loc='upper left', borderpad=0, borderaxespad=0)
ax.set_xlim((1, 9))
ax.set_xticks([1,3,5,7,9])
ax.set_ylim((1, 2**12))
breathe(ax, which='y')
ax.spines.bottom.set_position(('data', 0.5))  # breathe does not work on log plots
mktrans(fig, ax)
display(fig)
```

### Computations of average variation

To compute (a bound on) $V_n$ for some mappings, we consider the binary expansion of $\alpha$ and $\alpha^\prime$ which appear in the supremum. For clarity and to represent actual numerical implementations, we furthermore truncate the binary expansion to $2m$ bits (on most machines, $2m=64$ bits).

$$
\begin{aligned}
\alpha &= 0. b_1 b_2 \cdots b_{2m-1} b_{2m}\\
\alpha^\prime &= 0. b_1^\prime b_2^\prime \cdots b_{2m-1}^\prime b_{2m}^\prime \\
\end{aligned}
$$

Furthermore, since $\alpha \in \left[\frac{i-1}{2^n}, \frac{i}{2^n} \right],\, i \in \{1, \cdots, 2^n\}$, we may express the first $n$ (where $n < 2m$) bits of $\alpha$ with the bits of $i=\sum_{k=0}^{n-1} i_k 2^k$ (we ignore $i=2^n$ for the sake of simplicity, which would introduce one more bit, $n+1$ in total).

$$
\begin{aligned}
\alpha &= 0. i_{n-1} i_{n-2} \cdots i_1 i_0 b_n b_{n+1} \cdots b_{2m-1} b_{2m}\\
\alpha^\prime &= 0. i_{n-1} i_{n-2} \cdots i_1 i_0 b^\prime_n b^\prime_{n+1} \cdots b^\prime_{2m-1} b^\prime_{2m}
\end{aligned}
$$

#### Z-mapping

Let us consider the Z-mapping, and the two terms compositing its inverse. In the case of $\alpha, \alpha^\prime$ above, we have if $n$ is even:

$$
\begin{aligned}
Z^{-1}_1(\alpha) &= 0.\underbrace{i_{n-1} i_{n-3} \cdots i_1}_\text{$n/2$ bits} b_n b_{n+2} \cdots b_{2m-1} \\
Z^{-1}_2(\alpha) &= 0.\underbrace{i_{n-2} i_{n-4} \cdots i_0}_\text{$n/2$ bits} b_{n+1} b_{n+3} \cdots b_{2m}.
\end{aligned}
$$

If $n$ is odd, we have:

$$
\begin{aligned}
Z^{-1}_1(\alpha) &= 0.\underbrace{i_{n-1} i_{n-3} \cdots i_0}_\text{$(n+1)/2$ bits} b_n b_{n+2} \cdots b_{2m-1} \\
Z^{-1}_2(\alpha) &= 0.\underbrace{i_{n-2} i_{n-4} \cdots i_1}_\text{$(n-1)/2$ bits} b_{n+1} b_{n+3} \cdots b_{2m}.
\end{aligned}
$$

We only consider even $n$ in the following, as the proof for the latter remains very similar.

The core of the proof relies on the fact the first $n/2$ bits of the difference of the inverses cancel out when computing the average binned variation.

$$
\begin{aligned}
|Z^{-1}_1(\alpha) - Z^{-1}_1(\alpha^\prime)| &= \left|0.0 \cdots 0 (b_n b_{n+2} \cdots b_{2m-1} - b^\prime_n b^\prime_{n+2} \cdots b^\prime_{2m-1})\right| \\
&= \left|2^{-n/2} \sum_{k=1}^{\tfrac{2m-n+1}{2}} (b_{n+2k-1} - b^\prime_{n+2k-1}) 2^{-k}\right| \\
&\leq 2^{-n/2} \\
|Z^{-1}_2(\alpha) - Z^{-1}_2(\alpha^\prime)| &= \left|2^{-n/2} \sum_{k=1}^{\tfrac{2m-n+1}{2}} (b_{n+2k} - b^\prime_{n+2k}) 2^{-k}\right| \\
&\leq 2^{-n/2}
\end{aligned}
$$

Putting everything together, we get a vanishing upper bound for the average binned variation.

$$
V_n(Z^{-1}) \leq \frac{1}{2^n} \sum_{i=1}^{2^n} 2^{-n/2} + 2^{-n/2} = 2^{-n/2+1} \xrightarrow{n \to \infty} 0
$$

#### Column mapping

We now consider the Column mapping. Since here we consider finite-precision $\alpha$ and $\alpha^\prime$, the invere is well-defined and we have

$$
\begin{aligned}
C^{-1}_1(\alpha) &= 0.i_{n-1} i_{n-2} \cdots i_1 i_0 b_n b_{n+1} \cdots b_m \\
C^{-1}_2(\alpha) &= 0.b_{m+1} b_{m+2} \cdots b_{2m}.
\end{aligned}
$$

The first component can be bounded using the same trick from the previous proof :

$$
\begin{aligned}
|C^{-1}_1(\alpha) - C^{-1}_1(\alpha^\prime)| &= \left| 2^{-n} \sum_{k=1}^{m-n+1} (b_{n+k-1} - b^\prime_{n+k-1}) 2^{-k} \right| \\
&\leq 2^{-n}.
\end{aligned}
$$

However, now problems arise when we compute the averge binned variation, since the second component is not well-behaved. In particular, since we take the supremum on $\alpha,\alpha^\prime \in \left[\tfrac{i-1}{2^n},\tfrac{i}{2^n}\right]^2$, we can pick the specific values such that $b_k=1$ and $b^\prime_k=0$ for all $k \in \{m+1, \cdots 2m\}$.

$$
\begin{aligned}
|C^{-1}_2(\alpha) - C^{-1}_2(\alpha^\prime)| &= \left| \sum_{k=m+1}^{2m} (b_k - b^\prime_k) 2^{-(k-m)} \right| \\
&= \sum_{k=m+1}^{2m} 2^{-(k-m)} \\
&> \frac 12
\end{aligned}
$$

Putting both together, we can write:

$$
\begin{aligned}
V_n(C^{-1}) &= \frac{1}{2^n}\sum_{i=1}^{2^n} \sup_{\alpha, \alpha^\prime \in \left[\tfrac{i-1}{2^n}, \tfrac{i}{2^n}\right]^2} |C^{-1}_1(\alpha^\prime) - C_1^{-1}(\alpha)| + |C^{-1}_2(\alpha^\prime) - C_2^{-1}(\alpha)| \\
&\geq
\begin{aligned}[t]
	\frac{1}{2^n}\sum_{i=1}^{2^n}&
	\sup_{\alpha, \alpha^\prime \in \left[\tfrac{i-1}{2^n}, \tfrac{i}{2^n}\right]^2} |C^{-1}_1(\alpha^\prime) - C_1^{-1}(\alpha)|\\
	&+ \sup_{\alpha, \alpha^\prime \in \left[\tfrac{i-1}{2^n}, \tfrac{i}{2^n}\right]^2} |C^{-1}_2(\alpha^\prime) - C_2^{-1}(\alpha)|
\end{aligned}\\
&\geq \frac 12 + \bO(2^{-n}).
\end{aligned}
$$

Therefore we have found a lower bound, which proves the average binned variation does not converge to zero (if it converges at all).

$$
V_n(C^{-1}) > \frac 12 \implies \lim_{n \to \infty} V_n(C^{-1}) > \frac 12
$$

## Numerical convergence of the finite-$n$ estimations

<!-- In @sec-coarse-graining we discussed how we approximate the fractal mappings numerically, and introduced the coarse-graining procedure, in which we average the populations in segments of length $2^{-n}$, and end up with $2^n$ "binned" segment populations, compared to the $4^n$ we started with.

By doing this, we were able to numerically estimate the integral on the one-dimensional fractal space, and showed that local mappings exhibited the property that they conserved the dynamics between two spaces of different dimensions. @sec-Vn introduced the analytical metric $V_n$ of this locality. Assuming that $V_n \xrightarrow{n\to\infty} 0$, is this enough to prove that the numerical estimation of the integral goes to its analytical value? -->

In @sec-Vn, we have introduced the average variation metric $V_n$. Assuming that $V_n(S^{-1}) \xrightarrow{n\to\infty} 0$ (which expresses the locality property), does this imply that the numerical estimation of the integral in the neural field equation in $[0,1]$ converges to its true value?

The intuition behind the proof of convergence is that the variation, coming from the coarse-graining, inside each numerical bin vanishes as we increase the number of iterations $n$, even though the neural field $\tilde{h}$ and the connectivity kernel $\tilde{w}$ become fractal.

We define the one-dimensional field on the grid:

$$
\tilde h_i(t) = \tilde h(\tfrac{i-1}{2^n}, t) = \tilde h(\beta_i, t),\,\beta_i=\tfrac{i-1}{2^n},\,i\in\{1,\cdots,2^n\}.
$$

Let us first recall the numerical and analytical integral. For any $\alpha \in [0,1]$, let us define:
$$
\begin{aligned}
\mathrm{NI}_n &= \frac{1}{2^n} \sum_{i=1}^{2^n} \tilde w(\alpha, \beta_i) \phi(\tilde h_i(t)) \\
\mathrm{AI} &= \int_{[0,1]} \tilde w(\alpha, \beta) \phi(\tilde h(\beta, t)) \left[\lambda \circ S^{-1}\right](\d \beta).
\end{aligned}
$$

We wish to prove that $|\mathrm{NI}_n - \mathrm{AI}| \xrightarrow{n \to \infty} 0$.

The first part of the proof involves splitting the integral on $[0,1]$ into $2^n$ integral over segments of length $2^{-n}$, and using the triangle inequality.

$$
\begin{aligned}
|\mathrm{NI}_n - \mathrm{AI}| &= \left| \frac{1}{2^n} \sum_{i=1}^{2^n} \tilde w(\alpha, \beta_i) \phi(\tilde h_i(t)) - \int_{0}^{1} \tilde w(\alpha, \beta) \phi(\tilde h(\beta, t)) \left[\lambda \circ S^{-1}\right](\d \beta) \right| \\
&= \left| \frac{1}{2^n} \sum_{i=1}^{2^n} \tilde w(\alpha, \beta_i) \phi(\tilde h_i(t)) - \sum_{i=1}^{2^n} \int_{\tfrac{i-1}{2^n}}^{\tfrac{i}{2^n}} \tilde w(\alpha, \beta) \phi(\tilde h(\beta, t)) \left[\lambda \circ S^{-1}\right](\d \beta) \right| \\
&= \left| \sum_{i=1}^{2^n} \frac{1}{2^n} \tilde w(\alpha, \beta_i) \phi(\tilde h_i(t)) - \int_{\tfrac{i-1}{2^n}}^{\tfrac{i}{2^n}} \tilde w(\alpha, \beta) \phi(\tilde h(\beta, t)) \left[\lambda \circ S^{-1}\right](\d \beta) \right| \\
&\leq \sum_{i=1}^{2^n} \left| \frac{1}{2^n} \tilde w(\alpha, \beta_i) \phi(\tilde h_i(t)) - \int_{\tfrac{i-1}{2^n}}^{\tfrac{i}{2^n}} \tilde w(\alpha, \beta) \phi(\tilde h(\beta, t)) \left[\lambda \circ S^{-1}\right](\d \beta) \right| \\
\end{aligned}
$$

We now want to prove that evaluating the integrand at $\beta_i = \tfrac{i-1}{2^n}$ gives a good approximation of the integral. Let us assume that $S$ is the Z-mapping. Then, $S$ is measure-preserving (if $\lambda$ is the Lebesgue measure on $[0,1]^2$, then $\lambda \circ S^{-1}$ is the Lebesgue measure on $[0,1]$). This implies that we have $\int_{\tfrac{i-1}{2^n}}^{\tfrac{i}{2^n}} \left[\lambda \circ S^{-1}\right](\d \beta) = \tfrac{1}{2^n}$, and we write

$$
\begin{aligned}
|\mathrm{NI}_n - \mathrm{AI}| &\leq \sum_{i=1}^{2^n} \left| \int_{\tfrac{i-1}{2^n}}^{\tfrac{i}{2^n}} \Delta(\beta) \left[\lambda \circ S^{-1}\right](\d \beta) \right| \\
\text{where}\ \Delta(\beta) &= \tilde w(\alpha, \beta_i) \phi(\tilde h_i(t)) - \tilde w(\alpha, \beta) \phi(\tilde h(\beta, t))\\
&= w_U(\alpha, S^{-1}(\beta_i)) \phi(h_U(S^{-1}(\beta_i), t)) - w_U(\alpha, S^{-1}(\beta)) \phi(h_U(S^{-1}(\beta), t)).
\end{aligned}
$$

The main source of variance inside the integrand $\Delta(\beta)$ is the term $S^{-1}$, since $w_U$ and $h_U$ are "nice" functions. We express this regularity by the added assumption that $w_U(\alpha, \vec u) \phi(h_U(t, \vec u))$ is Lipschitz in $\vec u$, with Lipschitz constant $L(\alpha)$, and $\norm{\alpha}$ is the corresponding norm on $[0,1]^2$. We argue this is not a strong assumption, because this is just expressing the regularity of the integrand in the integral over $[0,1]^2$.

A more pragmatic justification of this is to notice that the "density of recurrent currents" is expressed as:

$$
I^\text{rec}_U(\vec v, \vec u) = w_U(\vec v, \vec u) \phi(h_U(t, \vec u)),
$$

and that in @sec-cycling-nf, numerical simulations showed that $I^\text{rec}_U(\vec v, \vec u)$ (the integrand of the integral on $[0,1]^2$) is a well-behaved function.

Applying the Lipschitz assumption yields the following inequality on the integrand:

$$
\begin{aligned}
\Delta(\beta) &= w_U(\alpha, S^{-1}(\beta_i)) \phi(h_U(S^{-1}(\beta_i), t)) - w_U(\alpha, S^{-1}(\beta)) \phi(h_U(S^{-1}(\beta), t)) \\
&\leq L(\alpha) \norm{S^{-1}(\beta_i) - S^{-1}(\beta)}.
\end{aligned}
$$

We now make use of our intuition of "locality": points close in $[0,1]$ should be (on average) mapped to points close in $[0,1]^2$. Therefore, taking the sup inside each $2^{-n}$ segment does not introduce a too large error.

$$
\begin{aligned}
|\mathrm{NI}_n - \mathrm{AI}| &\leq L(\alpha) \sum_{i=1}^{2^n} \left| \int_{\tfrac{i-1}{2^n}}^{\tfrac{i}{2^n}} \norm{S^{-1}(\beta_i) - S^{-1}(\beta)} \left[\lambda \circ S^{-1}\right](\d \beta) \right| \\
&\leq L(\alpha) \sum_{i=1}^{2^n} \left| \int_{\tfrac{i-1}{2^n}}^{\tfrac{i}{2^n}} \sup_{\beta \in \left[\tfrac{i-1}{2^n}, \tfrac{i}{2^n}\right]} \norm{S^{-1}(\beta_i) - S^{-1}(\beta)} \left[\lambda \circ S^{-1}\right](\d \beta^\prime) \right| \\
&= L(\alpha) \frac{1}{2^n} \sum_{i=1}^{2^n} \sup_{\beta \in \left[\tfrac{i-1}{2^n}, \tfrac{i}{2^n}\right]} \norm{S^{-1}(\beta_i) - S^{-1}(\beta)} \\
&\leq L(\alpha) \frac{1}{2^n} \sum_{i=1}^{2^n} \sup_{\beta, \beta^\prime \in \left[\tfrac{i-1}{2^n}, \tfrac{i}{2^n}\right]^2} \norm{S^{-1}(\beta^\prime) - S^{-1}(\beta)} \\
&= L(\alpha) V_n(S^{-1}) \\
&\xrightarrow{n \to \infty} 0
\end{aligned}
$$

We see that it is the notion of "locality" of the mapping $S$ that allowed us to write the convergence of the numerical integral to the analytical value. In doing so, we exploited the fact that the "variance" inside each small segment in $[0,1]$ is small on average, and that the errors vanish as we take finer and finer bins.

## Generalizing to $[0,1]^p \mapsto [0,1]$

We finish this thesis by tying up the loose ends. In the abstract, we wrote that any $p$-dimensional neural field equation is equivalent to a neural field equation in $[0,1]$, but we only gave demonstrations for $[0,1]^2$ to $[0,1]$. @sec-simulations-3d-2d-1d showed that it is possible to iteratively apply the mapping $Z : [0,1]^2 \mapsto [0,1]$ to reduce dimensionality of the neural field, but it might be more practical to have a direct mapping $Z_p : [0,1]^p \mapsto [0,1]$.

Let us give a generalization of the Z-mapping. Defining $\vec{v^{(n)}} = (v^{(n)}_1, \cdots, v^{(n)}_p)$, and the finite binary expansions $v^{(n)}_\mu = \sum_{k=1}^{n} b^\mu_k 2^{-k}$, we can define $Z^n_p$ as

$$
\begin{aligned}
Z_p^n(\vec{v^{(n)}}) &= \sum_{k=1}^{n} \sum_{\mu=1}^p b^{\mu}_k 2^{-(p(k-1)+\mu)} \\
&= 0.b^1_1 b^2_1 \cdots b^p_1 b^1_2 b^2_2 \cdots b^p_{n-1} b^1_n b^2_n \cdots b^p_n.
\end{aligned}
$$

The same arguments apply to show that this generalized Z-mapping converges pointwise to $Z_p$, has the locality property, and that $V_n(Z_p^{-1}) \xrightarrow{n\to\infty} 0$. Then the proof of convergence, albeit more laborious, can be repeated using the same procedure.