# Appendix

::: {.content-hidden}
{{< include ../macros.tex >}}
:::

## Fixed point study in the network of neurons

We recall the equation for the low-rank network of neurons :

$$
\dot h_i(t) = -h_i(t) + \frac 1N \sum_{\mu=1}^p \sum_{j=1}^N F_{\mu i} G_{\mu j} \phi(h_j(t))
$$

The low-rank vectors are in general obtained by sampling a distribution $\vec{\xi} = (\xi_1, \cdots, \xi_p) \sim \rho$. Every neuron $i$ samples this distribution independantly, and low-rank vectors are obtained by passing the samples through functions $f_\mu(\vec{\xi})$ and $g_\mu(\vec{\xi})$.

We note that in the main text, the distribution is a multivariate normal with unit covariance and zero mean, such that it factors out as $\rho(\xi_1, \cdots, \xi_p) = \prod_{\mu=1}^p \mathcal{N}(\xi_\mu)$. Furthermore, the functions are component-wise $f_\mu(\xi_1, \cdots, \xi_p) = \xi_\mu$ and $g_\mu(\xi_1, \cdots, \xi_p) = \tilde \phi(\xi_\mu) = \frac{\phi(\xi_\mu) - \avg{\phi(\xi_\mu)}}{\mathrm{Var}[\phi(\xi_\mu)]}$.

$$
F_{\mu i} = f_\mu(\xi_{1 i}, \cdots, \xi_{p i}), \, G_{\mu i} = g_\mu(\xi_{1 i}, \cdots, \xi_{p i})
$$

### Derivation of fixed points

In order to find the fixed points, one has to solve in general $\dot h_i(t) = 0$ for all $i = 1, \cdots, N$. Because of the nonlinearity $\phi(h_j(t))$, this is difficult, but we can guess the fixed points $h^\star_i$ and verify them by substituting into the equation that $\dot h_i(t) = 0$.

#### Zero fixed point

A trivial fixed point is $h^\star_i = 0$ for all $i$, which we prove to be a fixed point as $N \to \infty$.

$$
\begin{aligned}
\dot h_i(t) &= \frac 1N \sum_{\mu=1}^p \sum_{j=1}^N F_{\mu i} G_{\mu j} \phi(0) \\
&= \frac{\phi(0)}{N} \sum_{\mu=1}^p \sum_{j=1}^N F_{\mu i} G_{\mu j} \\
&= \frac{\phi(0)}{N} \sum_{\mu=1}^p \sum_{j \neq i}^N F_{\mu i} G_{\mu j} + \underbrace{\frac{\phi(0)}{N} \sum_{\mu=1}^p F_{\mu i} G_{\mu i}}_{\bO(\frac 1N)} \\
&= \phi(0) \sum_{\mu=1}^p \frac 1N \sum_{j \neq i}^N F_{\mu i} G_{\mu j} + \bO(\tfrac 1N) \\
&= \phi(0) \sum_{\mu=1}^p F_{\mu i} \frac{N-1}{N} \frac{1}{N-1} \sum_{j \neq i}^N G_{\mu j} + \bO(\tfrac 1N) \\
&\stackrel{N \gg 1}{\approx} \phi(0) \sum_{\mu=1}^p F_{\mu i} \avg{g_\mu(\vec{\xi})}
\end{aligned}
$$

<!-- TODO : fixme. the F_i should not be in the integral -->

To write the last line, we make a slight abuse of notation in order to avoid notation problems when taking the $N \to \infty$ limit of $F_{\mu i}$.

In the case of the network studied in this work, the find that

$$
\avg{g_\mu(\vec{\xi})} = \avg{\tilde \phi(\xi_\mu)} = \avg{\frac{\phi(\xi_\mu) - \avg{\phi(\xi_\mu)}}{\mathrm{Var}[\phi(\xi_\mu)]}} = 0
$$

Thus, $h_i^\star=0$ is a fixed point in the limit $N \to \infty$.

#### Pattern fixed point

Another set of fixed points is $h^\star_i = F_{\nu i}$ for all $\nu = 1, \cdots, p$. We refer to these as the "patterns" in the context of learning the network "rembers" (converges to) these vectors. We verify the fixed point similarly as before.

$$
\begin{aligned}
\dot h_i(t) &= -F_{\nu i} + \frac 1N \sum_{\mu=1}^p \sum_{j=1}^N F_{\mu i} G_{\mu j} \phi(F_{\nu j}) \\
&= -F_{\nu i} + \frac 1N \sum_{\mu=1}^p \sum_{j \neq i}^N F_{\mu i} G_{\mu j} \phi(F_{\nu j}) + \bO(\tfrac 1N) \\
&= -F_{\nu i} + \frac 1N \sum_{j \neq i}^N F_{\nu i} G_{\nu j} \phi(F_{\nu j}) + \frac 1N \sum_{\mu \neq \nu}^p \sum_{j \neq i}^N F_{\mu i} G_{\mu j} \phi(F_{\nu j}) + \bO(\tfrac 1N) \\
&\stackrel{N \gg 1}{\approx} -F_{\nu i} + F_{\nu i} \avg{g_\nu(\vec{\xi_j}) \phi(\xi_{\nu j})} + \sum_{\mu \neq \nu}^p F_{\mu i} \avg{g_\mu(\vec{\xi_j}) \phi(\xi_{\nu j})}
\end{aligned}
$$

The term $\avg{g_\mu(\vec{\xi_j}) \phi(\xi_{\nu j})}$ vanishes in the case of a factorizable distribution and when the functions $g_\mu$ are component-wise.

$$
\begin{aligned}
\avg{g_\mu(\vec{\xi_j}) \phi(\xi_{\nu j})} &= 
\int_{\Rp} g_\mu(\xi_1, \cdots, \xi_p) \phi(\xi_\nu) \rho(\d \vec{\xi}) \\
&= \int_{\Rp} g_\mu(\xi_\mu) \phi(\xi_\nu) \prod_{\gamma=1}^p \rho_\gamma(\d \xi_\gamma) \\
&= \int_{\R} g_\mu(\xi_\mu) \rho_\mu(\d \xi_\mu) \int_{\R} \phi(\xi_\nu) \rho_\nu(\d \xi_\nu) \\
&= \avg{g_\mu(\xi_\mu)} \avg{\phi(\xi_\nu)}
\end{aligned}
$$

As established in the previous section, in the case of the network studied in this work, $\avg{g_\mu(\xi_\mu)} = 0$.

The term $\avg{g_\nu(\vec{\xi_j}) \phi(\xi_{\nu j})}$ is more difficult, but we can prove that in our case it equals one.

$$
\begin{aligned}
\avg{g_\nu(\vec{\xi_j}) \phi(\xi_{\nu j})} &=
\avg{\frac{\phi(\xi_{\mu j}) - \avg{\phi(\xi_{\mu j})}}{\mathrm{Var}[\phi(\xi_{\mu j})]} \phi(\xi_{\mu j})} \\
&= \avg{\frac{\phi(\xi)^2 - \avg{\phi(\xi)} \phi(\xi) }{\mathrm{Var}[\phi(\xi)]}} \\
&= \frac{\avg{\phi(\xi)^2} - \avg{\avg{\phi(\xi)} \phi(\xi)} }{\mathrm{Var}[\phi(\xi)]} \\
&= \frac{\avg{\phi(\xi)^2} - \avg{\phi(\xi)}^2}{\mathrm{Var}[\phi(\xi)]} \\
&= \frac{\mathrm{Var}[\phi(\xi)]}{\mathrm{Var}[\phi(\xi)]} \\
&= 1
\end{aligned}
$$

Therefore, the set of $h_i^\star = F_{\nu i}$ for all $\nu = 1, \cdots, p$ are fixed points in the limit of large $N$, under the assumptions that the distribution is factorizable, the functions $g_\mu$ are component-wise, and $\avg{g_\nu(\xi) \phi(\xi)}=1$, which is the case in the setup of this work.

### Fixed points of the neural field

The derivation of the fixed points for the neural field equation follows the same arguments as the fixed points in the network of neurons.

As $N \to \infty$, the fixed points $h_i^\star$ become functions $h^\star(\xi_1, \cdots, \xi_p)$ on the $\Rp$ space :

* $h_i^\star = 0 \to h^\star(\xi_1, \cdots, \xi_p) = 0$
* $h_i^\star = F_{\nu i} = \xi_{\nu i} \to h^\star(\xi_1, \cdots, \xi_p) = \xi_\nu$ for all $\nu = 1, \cdots, p$

<!-- TODO : add derivation for neural field ? -->

### Stability of the fixed points

<!-- TODO -->

## Behavior of $\kappa_\mu(t)$ in the cyclic network

kappa in the delayed, rolled and cyclic network

relation to overlaps

## "Mean patterns" in the binned connectivity

derivation of $\Jab$ and mean patterns

For finite number of recursive quadrant iterations $n$, we can do a "mean-field approximation" inside each of the $4^n$ segments. Let $\alpha = \{i_1,\cdots,i_{|\alpha|}\}$ be the multi-index corresponding to all neurons of which the embedding in $\mathbb R^p$ gets mapped to the segment $\alpha$ in $[0,1]$. Let $H_\alpha(t) = \frac 1 {|\alpha|} \sum_{i \in \alpha} h_i(t)$ be the (mean) RNN potential of the segment $\alpha$. The connectivity matrix $\tilde J_{\alpha,\beta}$ satisfies

$$
\dot H_\alpha(t) = -H_\alpha(t) + \sum_{\beta \in \text{segments of length } 4^{-n}} \tilde J_{\alpha,\beta} \phi(H_\beta(t))
$$

By substituting the original $h_i(t)$, we find the correct rescaling is given by

$$
\tilde J_{\alpha,\beta} = \frac 1 {|\alpha|} \sum_{i \in \alpha} \sum_{j \in \beta} J_{ij}
$$

In the low-rank case, we have

$$
J_{ij} = \frac 1N \sum_{\mu=1}^p F_{\mu,i} G_{\mu,j}
$$

We can define the "mean pattern" inside each bin :

$$
\tilde F_{\mu,\alpha} = \frac{1}{|\alpha|} \sum_{i \in \alpha} F_{\mu,i}, \; \tilde G_{\mu,\alpha} = \frac{1}{|\alpha|} \sum_{i \in \alpha} G_{\mu,i}
$$

Then the connectivity matrix is given by, noting $N = \sum_{\alpha} |\alpha|$,

$$
\tilde J_{\alpha,\beta} = \frac{|\beta|}{\sum_{\beta'} |\beta'|} \left( \sum_{\mu=1}^p \tilde F_{\mu,\alpha} \tilde G_{\mu,\beta} - \delta_{\alpha,\beta} \underbrace{\sum_{\mu=1}^p \sum_{i \in \alpha} \frac{F_{\mu,i}}{|\alpha|} \frac{G_{\mu,i}}{|\alpha|}}_{\gamma_{\alpha}} \right)
$$

Recurrent current is given by

$$
I^\text{rec}_\alpha(t) = \sum_\beta \tilde J_{\alpha,\beta} \phi(H_\beta(t)) = \frac{1}{\sum_{\beta'} |\beta'|} \left( \sum_\beta |\beta| \sum_{\mu=1}^p \tilde F_{\mu,\alpha} \tilde G_{\mu,\beta} \phi(H_\beta(t)) - |\alpha| \tilde \gamma_\alpha \phi(H_\alpha(t)) \right)
$$

The overlaps can be computed as

$$
\tilde m_\mu(t) = \frac{1}{\sum_{\alpha} |\alpha|} \sum_\alpha |\alpha| G_{\mu,\alpha} \phi(H_\alpha(t))
$$
