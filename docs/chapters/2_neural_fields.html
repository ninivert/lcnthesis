<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.361">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Equivalence of neural field dynamics with different embedding dimensionality - 2&nbsp; Neural field toy model and simulations</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../chapters/3_mappings.html" rel="next">
<link href="../chapters/1_intro.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../chapters/2_neural_fields.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Neural field toy model and simulations</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Equivalence of neural field dynamics with different embedding dimensionality</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/ninivert/lcnthesis" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="../Equivalence-of-neural-field-dynamics-with-different-embedding-dimensionality.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Abstract</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/1_intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/2_neural_fields.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Neural field toy model and simulations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/3_mappings.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Mapping neural fields to lower-dimensional embeddings</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/4_locality.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Locality and numerical convergence</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/5_conclusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Conclusion</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../acknowledgements.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acknowledgements</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendices/A_fixed_points.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Fixed point study in the network of neurons</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendices/B_mean_patterns.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Binned sampling of populations and visualization of kernels</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-networks-of-neurons-to-neural-fields" id="toc-sec-networks-of-neurons-to-neural-fields" class="nav-link active" data-scroll-target="#sec-networks-of-neurons-to-neural-fields"><span class="header-section-number">2.1</span> Networks of neurons to neural fields</a>
  <ul>
  <li><a href="#sec-low-rank-network" id="toc-sec-low-rank-network" class="nav-link" data-scroll-target="#sec-low-rank-network"><span class="header-section-number">2.1.1</span> Low-rank networks of neurons</a></li>
  <li><a href="#gaussian-low-rank-network-of-neurons" id="toc-gaussian-low-rank-network-of-neurons" class="nav-link" data-scroll-target="#gaussian-low-rank-network-of-neurons"><span class="header-section-number">2.1.2</span> Gaussian low-rank network of neurons</a></li>
  <li><a href="#emergence-of-structure-in-the-networks-of-neurons" id="toc-emergence-of-structure-in-the-networks-of-neurons" class="nav-link" data-scroll-target="#emergence-of-structure-in-the-networks-of-neurons"><span class="header-section-number">2.1.3</span> Emergence of structure in the networks of neurons</a>
  <ul class="collapse">
  <li><a href="#numerical-aspects-of-simulating-a-low-rank-network-of-neurons" id="toc-numerical-aspects-of-simulating-a-low-rank-network-of-neurons" class="nav-link" data-scroll-target="#numerical-aspects-of-simulating-a-low-rank-network-of-neurons"><span class="header-section-number">2.1.3.1</span> Numerical aspects of simulating a low-rank network of neurons</a></li>
  <li><a href="#naive-and-natural-embeddings-for-p1" id="toc-naive-and-natural-embeddings-for-p1" class="nav-link" data-scroll-target="#naive-and-natural-embeddings-for-p1"><span class="header-section-number">2.1.3.2</span> Naive and natural embeddings for <span class="math inline">\(p=1\)</span></a></li>
  <li><a href="#a-natural-embedding-for-p2" id="toc-a-natural-embedding-for-p2" class="nav-link" data-scroll-target="#a-natural-embedding-for-p2"><span class="header-section-number">2.1.3.3</span> A natural embedding for <span class="math inline">\(p=2\)</span></a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sec-simnf" id="toc-sec-simnf" class="nav-link" data-scroll-target="#sec-simnf"><span class="header-section-number">2.2</span> Simulation schemes for neural fields</a>
  <ul>
  <li><a href="#sec-mc" id="toc-sec-mc" class="nav-link" data-scroll-target="#sec-mc"><span class="header-section-number">2.2.1</span> Sampling method (Monte Carlo integration)</a></li>
  <li><a href="#sec-gridmethod" id="toc-sec-gridmethod" class="nav-link" data-scroll-target="#sec-gridmethod"><span class="header-section-number">2.2.2</span> Grid method (Trapezoidal integration)</a></li>
  </ul></li>
  <li><a href="#characterizing-dynamics-of-the-low-rank-neural-field" id="toc-characterizing-dynamics-of-the-low-rank-neural-field" class="nav-link" data-scroll-target="#characterizing-dynamics-of-the-low-rank-neural-field"><span class="header-section-number">2.3</span> Characterizing dynamics of the low-rank neural field</a>
  <ul>
  <li><a href="#overlap-variables" id="toc-overlap-variables" class="nav-link" data-scroll-target="#overlap-variables"><span class="header-section-number">2.3.1</span> Overlap variables</a></li>
  <li><a href="#sec-closed-system" id="toc-sec-closed-system" class="nav-link" data-scroll-target="#sec-closed-system"><span class="header-section-number">2.3.2</span> Low-rank neural fields as <span class="math inline">\(p\)</span>-dimensional closed systems</a></li>
  </ul></li>
  <li><a href="#sec-cycling-nf" id="toc-sec-cycling-nf" class="nav-link" data-scroll-target="#sec-cycling-nf"><span class="header-section-number">2.4</span> A cycling neural field</a></li>
  <li><a href="#sec-mapping-cdf" id="toc-sec-mapping-cdf" class="nav-link" data-scroll-target="#sec-mapping-cdf"><span class="header-section-number">2.5</span> Mapping <span class="math inline">\(\mathbb R^2\)</span> to <span class="math inline">\([0,1]^2\)</span> using the CDF</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-nf" class="quarto-section-identifier"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Neural field toy model and simulations</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>In this chapter, we introduce a toy model so that we can later study the mappings of embedding spaces. We stress that this toy model serves only for illustration purposes, and our results apply to any neural field. For this reason, we only cite or give short derivations of results used to understand the model, and the influence of the mappings that are later applied to it.</p>
<section id="sec-networks-of-neurons-to-neural-fields" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="sec-networks-of-neurons-to-neural-fields"><span class="header-section-number">2.1</span> Networks of neurons to neural fields</h2>
<section id="sec-low-rank-network" class="level3" data-number="2.1.1">
<h3 data-number="2.1.1" class="anchored" data-anchor-id="sec-low-rank-network"><span class="header-section-number">2.1.1</span> Low-rank networks of neurons</h3>
<p>The toy model we introduce is an instance of a low-rank neural network. Such models hold their name from the form of their connectivity matrix, which has a rank <span class="math inline">\(p\)</span>. We write the connectivity matrix as follows:</p>
<p><span id="eq-lowrankj"><span class="math display">\[
J_{ij} = \frac 1N \sum_{\mu=1}^p F_{\mu i} G_{\mu j}.
\tag{2.1}\]</span></span></p>
<p>The geometric view of these networks is that the recurrent currents lie in a <span class="math inline">\(p\)</span>-dimensional subspace of <span class="math inline">\(\mathbb R^N\)</span> spanned by the linearly independant vectors <span class="math inline">\(\{\boldsymbol{F_1}, \cdots, \boldsymbol{F_\mu},\cdots, \boldsymbol{F_p}\}\)</span> (where each <span class="math inline">\(\boldsymbol{F_\mu}=(F_{\mu 1}, \cdots F_{\mu N})\)</span>), which therefore define a “natural” linear embedding (in the sense of <span class="citation" data-cites="JazOst21"><a href="../references.html#ref-JazOst21" role="doc-biblioref">[1]</a></span>) of the neural population activity.</p>
<p>Recalling the notation from the introduction, the <span class="math inline">\(h_i(t)\)</span> are neuron potentials (with initial condition <span class="math inline">\(h_i(0)\)</span>), <span class="math inline">\(\phi : \mathbb R\mapsto \mathbb R^+\)</span> is the monotonic increasing activation function, and we write the evolution equation for this network of neurons:</p>
<p><span id="eq-lowrank-evolution"><span class="math display">\[
\dot h_i(t) = \underbrace{-h_i(t)}_\text{exponential decay} + \underbrace{\frac 1N \sum_{\mu=1}^p \sum_{j=1}^{N} F_{\mu i} G_{\mu j} \phi(h_j(t))}_\text{recurrent current $I^\text{rec}_i(t)$}.
\tag{2.2}\]</span></span></p>
<p>The exponential decay term describes how, in isolation, a neuron’s potential tends to zero, biologically corresponding to the decay of the membrane potential to its value at rest. The recurrent current term <span class="math inline">\(I^\text{rec}_i(t)\)</span> describes the “currents” received by neuron <span class="math inline">\(i\)</span> from all the other neurons in the network.</p>
<p>We would like to add a few remarks on the physical units of <a href="#eq-lowrank-evolution">Equation&nbsp;<span>2.2</span></a>. Strictly speaking, a prefactor <span class="math inline">\(\tau\)</span> with units of <span class="math inline">\(\mathrm{seconds}^{-1}\)</span> should be multiplied with <span class="math inline">\(\dot h_i(t)\)</span> of units <span class="math inline">\(\mathrm{volt}\cdot\mathrm{seconds}^{-1}\)</span>, such that the left-hand side of the equation is consistent with the unit of voltage. Similarly, the activation function has units <span class="math inline">\(\mathrm{seconds}^{-1}\)</span> and the connectivity matrix units of <span class="math inline">\(\mathrm{coulomb}\)</span>, and a multiplicative term <span class="math inline">\(R\)</span> with units <span class="math inline">\(\mathrm{ohm}\)</span> should multiply the recurrent currents in order to yield a voltage. By rescaling time or connection weights, both <span class="math inline">\(\tau\)</span> and <span class="math inline">\(R\)</span> can be set to one, and so we omit to write them.</p>
<p>In our model we make the simplifying assumption that there is no external current (we only study the autonomous dynamics of the system), but this does not hurt the generalizability of our results.</p>
</section>
<section id="gaussian-low-rank-network-of-neurons" class="level3" data-number="2.1.2">
<h3 data-number="2.1.2" class="anchored" data-anchor-id="gaussian-low-rank-network-of-neurons"><span class="header-section-number">2.1.2</span> Gaussian low-rank network of neurons</h3>
<p>Let us now specify the low-rank model introduced in the previous section, and use the model introduced in <span class="citation" data-cites="schmutz2023convergence"><a href="../references.html#ref-schmutz2023convergence" role="doc-biblioref">[2]</a></span>, which can be seen as a simple example of a “Gaussian mixture low-rank network” <span class="citation" data-cites="Beiran2021"><a href="../references.html#ref-Beiran2021" role="doc-biblioref">[3]</a></span>. This paper defines the low-rank vectors <span class="math inline">\(F_{\mu i}\)</span> such that each component independently samples a standard Gaussian (zero mean, unit variance). In other words, every neuron <span class="math inline">\(i\)</span> samples a vector <span class="math inline">\(\boldsymbol{F_i} = (F_{1i}, \cdots, F_{pi})\)</span> from the <span class="math inline">\(p\)</span>-dimensional gaussian. We write</p>
<p><span class="math display">\[
\boldsymbol{F_i} = \boldsymbol{z_i},\ \boldsymbol{z_i} \stackrel{\text{iid}}{\sim} \rho(z_1, \cdots, z_p),
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\begin{aligned}
\rho(z_1, \cdots, z_p) &amp;= \prod_{\mu=1}^p \mathcal{N}(z_\mu) \\
\mathcal{N}(z) &amp;= \frac{1}{\sqrt{2 \pi}} \mathrm{e}^{-\frac 12 z^2}.
\end{aligned}
\]</span></p>
<p>The vectors <span class="math inline">\(G_{\mu i}\)</span> are defined as</p>
<p><span class="math display">\[
G_{\mu i} = \tilde\phi(z_{\mu i}) \stackrel{\text{def}}{=} \frac{\phi(z_{\mu i}) - \langle{\phi(z_{\mu i})}\rangle}{\mathrm{Var}[\phi(z_{\mu i})]}.
\]</span></p>
<p>The motivation for this choice will be presented in a few paragraphs. Finally, the activation function remains to be defined. For simplicity, we take it to be the logistic function, although the following results are not sensitive to this choice,</p>
<p><span class="math display">\[
\phi(h) = \frac{1}{1 + \mathrm{e}^{-h}}.
\]</span></p>
<p>Putting everything together, we get the following expression for the equation of evolution of the network of neurons:</p>
<p><span id="eq-toymodel-evolution"><span class="math display">\[
\dot h_i(t) = -h_i(t) + \frac 1N \sum_{\mu=1}^p \sum_{j=1}^{N} z_{\mu i} \tilde \phi(z_{\mu j}) \phi(h_j(t)) =: \mathrm{RHS}(h_1, \cdots, h_N).
\tag{2.3}\]</span></span></p>
<p>We note that contrary to the model introduced in <span class="citation" data-cites="schmutz2023convergence"><a href="../references.html#ref-schmutz2023convergence" role="doc-biblioref">[2]</a></span>, which excludes the self-connection term by effectively setting <span class="math inline">\(J_{ii} = 0\)</span>, we do not do this, because it introduces a vanishing correction of order <span class="math inline">\(\mathcal{O}(1/N)\)</span>.</p>
<p>To understand this model better, we can analyze its fixed points and their stability. Here we summarize the results from the derivation presented in <a href="../appendices/A_fixed_points.html"><span>Appendix&nbsp;A</span></a>. We use the notation <span class="math inline">\(h^\star\)</span> to refer to fixed points of the network. The fixed points solve the roots of the evolution equation:</p>
<!--
$$
\dot h_i(t)\ \text{evaluated at}\ h_i^\star = -h^\star_i(t) + \frac 1N \sum_{\mu=1}^p \sum_{j=1}^{N} z_{\mu i} \tilde \phi(z_{\mu j}) \phi(h^\star_j(t)) = 0
$$ 
-->
<p><span class="math display">\[
\mathrm{RHS}(h^\star_1, \cdots, h^\star_N) = -h^\star_i + \frac 1N \sum_{\mu=1}^p \sum_{j=1}^{N} z_{\mu i} \tilde \phi(z_{\mu j}) \phi(h^\star_j) = 0
\]</span></p>
<ul>
<li><span class="math inline">\(h^\star_i = z_{\mu i}\)</span> is a stable fixed point for all <span class="math inline">\(\mu \in \{1,\cdots,p\}\)</span>. Additionally, because the Gaussian distribution is symmetric around zero, <span class="math inline">\(h^\star_i = -z_{\mu i}\)</span> are also stable fixed points. We refer to these fixed points as the “pattern” fixed points, because the network of neurons “remembers” the vectors <span class="math inline">\(\boldsymbol{F_1}, \cdots, \boldsymbol{F_p}\)</span>.</li>
<li><span class="math inline">\(h^\star_i=0\)</span> is an unstable fixed point. It corresponds to all the neuron potentials being set to zero.</li>
</ul>
<p>The analysis of stability from <a href="../appendices/A_fixed_points.html#sec-fixedpoints-stability"><span>Section&nbsp;A.3</span></a> can be summarized by the study of the eigenvalues of the matrix <span class="math inline">\(K_{ij} = J_{ij} \partial \phi(h^\star_j) - \mathrm{Id}_{ij}\)</span>.</p>
<ol type="1">
<li>Taking Taylor expansions of the activation functions reveals that – at least up to order 3 – uneven powers tend to increase stability of the pattern fixed points (and reciprocally, decrease stability of the zero fixed point), and vice-versa for the even powers (with the notable exception of the constant offset, which doesn’t play a role in this analysis).</li>
<li>A steeper slope at the origin of the activation function also seems to improve stability of the pattern fixed points. This corresponds to a sharper difference between the “inactive” (low potential) and the “active” (high potential) neurons.</li>
<li>The spectrum of <span class="math inline">\(K\)</span> is composed of <span class="math inline">\(p\)</span> eigenvalues (labeled <span class="math inline">\(\lambda_\mu\)</span> for all <span class="math inline">\(\mu\in\{1,\cdots,p\}\)</span>) that depend on the fixed point, and <span class="math inline">\(N-p\)</span> eigenvalues <span class="math inline">\(\lambda_\perp = -1\)</span> corresponding to the orthogonal complement of the pattern fixed points.</li>
</ol>
<p>In our case, this analysis in Taylor expansions was sufficient to explain the observed stability resulting from a logistic activation function.</p>
<p>We should however note that the results obtained in <a href="../appendices/A_fixed_points.html"><span>Appendix&nbsp;A</span></a> are valid in the <span class="math inline">\(N \to \infty\)</span> limit. A numerical analysis shows in <a href="#fig-spectrum">Figure&nbsp;<span>2.1</span></a> that at <span class="math inline">\(N=1024\)</span>, the numerical eigenvalues approximate the analytical eigenvalues, and this correspondence improves when we take larger <span class="math inline">\(N\)</span> (typically we will for the rest of this thesis take <span class="math inline">\(N &gt; 10^4\)</span>).</p>
<div id="fig-spectrum" class="cell quarto-layout-panel" data-execution_count="2">
<figure class="figure">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="cell-output cell-output-display quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-spectrum-1" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="2_neural_fields_files/figure-html/fig-spectrum-output-1.svg" class="img-fluid figure-img" data-ref-parent="fig-spectrum"></p>
<figcaption class="figure-caption">(a) Pattern (stable) fixed point</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-display quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-spectrum-2" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="2_neural_fields_files/figure-html/fig-spectrum-output-2.svg" class="img-fluid figure-img" data-ref-parent="fig-spectrum"></p>
<figcaption class="figure-caption">(b) Zero (unstable) fixed point</figcaption>
</figure>
</div>
</div>
</div>
<p></p><figcaption class="figure-caption">Figure&nbsp;2.1: Spectrum of <span class="math inline">\(K\)</span> for <span class="math inline">\(N=1024\)</span> and <span class="math inline">\(p=5\)</span>. Numerical estimations (black, every line represents the sorted spectrum of one random sample of <span class="math inline">\(K\)</span>, 20 samples total) of the eigenvalues are close to the analytical (<span class="math inline">\(N \to \infty\)</span>) derivation (red, only eigenvalues <span class="math inline">\(\lambda_\mu\)</span>). Eigenvalues above the dotted gray line (positive values) correspond to unstable fixed points, and below (negative values) correspond to stable fixed points. The first <span class="math inline">\(p\)</span> eigenvalues correspond to <span class="math inline">\(\lambda_\mu\)</span> (<span class="math inline">\(\mu\in\{1,\cdots,p\}\)</span>), and the remaining eigenvalues correspond to <span class="math inline">\(\lambda_\perp=-1\)</span>.</figcaption><p></p>
</figure>
</div>
</section>
<section id="emergence-of-structure-in-the-networks-of-neurons" class="level3" data-number="2.1.3">
<h3 data-number="2.1.3" class="anchored" data-anchor-id="emergence-of-structure-in-the-networks-of-neurons"><span class="header-section-number">2.1.3</span> Emergence of structure in the networks of neurons</h3>
<p>In the introduction, we introduced the convergence of a network of neurons to a smooth neural field when the embedding space is well-chosen. For our toy model, it seems natural to try to embed the neurons according to their positions on the <span class="math inline">\(p\)</span>-dimensional Gaussian.</p>
<section id="numerical-aspects-of-simulating-a-low-rank-network-of-neurons" class="level4" data-number="2.1.3.1">
<h4 data-number="2.1.3.1" class="anchored" data-anchor-id="numerical-aspects-of-simulating-a-low-rank-network-of-neurons"><span class="header-section-number">2.1.3.1</span> Numerical aspects of simulating a low-rank network of neurons</h4>
<p>We first demonstrate what happens with numerical simulations in the simple case of <span class="math inline">\(p=1\)</span>. We generate a network of 50,000 neurons, where each neuron samples a one-dimensional Gaussian to get its position in the embedding. The function <span class="math inline">\(\tilde \phi\)</span> is computed by rescaling <span class="math inline">\(\phi\)</span> with the numerically computed mean and variance.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p>Instead of numerically computing the full connectivity matrix <span class="math inline">\(J\)</span> (which would require storing <span class="math inline">\(N^2 = 50,000^2 \sim 10^{10}\)</span> entries, taking about 40 GiB if we use 32-bit floating point numbers… yikes!), we take advantage of its low-rank structure to compute recurrent currents, therefore only storing <span class="math inline">\(p \times N\)</span> numbers instead of <span class="math inline">\(N^2\)</span>.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
</section>
<section id="naive-and-natural-embeddings-for-p1" class="level4" data-number="2.1.3.2">
<h4 data-number="2.1.3.2" class="anchored" data-anchor-id="naive-and-natural-embeddings-for-p1"><span class="header-section-number">2.1.3.2</span> Naive and natural embeddings for <span class="math inline">\(p=1\)</span></h4>
<p>Initially, we set all the potentials to zero, then simulate the differential equation by using the Runge-Kutta of order 4 integration schema (see <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.solve_ivp.html"><code>scipy.integrate.solve_ivp</code></a>). In <a href="#fig-p1">Figure&nbsp;<span>2.2</span></a>, we show the resulting dynamics for a few thousand neurons. We order the neurons as they appear in the numerical array (a naive “random” one-dimensional embedding), and compare this to the ordering of neurons by their sampled position <span class="math inline">\(z_1 \sim \mathcal{N}\)</span>.</p>
<div id="fig-p1" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><video src="figures/embedding_p=1.mp4" class="img-fluid" controls=""><a href="figures/embedding_p=1.mp4">Video</a></video></p>
<figcaption class="figure-caption">Figure&nbsp;2.2: Simulation of the Gaussian network of neurons with <span class="math inline">\(p=1\)</span>. Every neuron is represented by a dot, of which the height represents its activity level, and the colorbar doubles as a y-axis.</figcaption>
</figure>
</div>
<!-- :::: {.content-visible when-format="pdf" layout-nrow=2}
:::{#fig-p1 layout-nrow=2}
![First frame](figures/embedding_p=1.mp4-1.jpg)

![Last frame](figures/embedding_p=1.mp4-2.jpg)

Simulation of the Gaussian network of neurons with $p=1$. Every neuron is represented by a dot, of which the height represents its activity level, and the colorbar doubles as a y-axis. The first and the last frame of the animation are shown, [available here](https://ninivert.github.io/lcnthesis/chapters/figures/embedding_p=1.mp4).
:::
:::: -->
<p>The results of the simulation show that the random ordering of the neurons does not yield a smooth embedding, in contrast to the smooth surface that appears when we order the neurons by their position <span class="math inline">\(z_{1i}\)</span>. The simulation serves as a demonstration that the neurons converge to the stable fixed point <span class="math inline">\(h^\star_i = z_{1i}\)</span>, as well as confirming that the zero fixed point is unstable. Indeed, despite the zero initialization, the simulation did not stay there because small numerical fluctuations were amplified by the instability.</p>
<p>The observed smooth surface corresponds to the activation function <span class="math inline">\(\phi(h^\star) = \phi(z_1)\)</span>. Therefore, the Gaussian patterns define a natural embedding of the network, and the neural field at the fixed point reads <span class="math inline">\(h^\star(z_1, t) = z_1\)</span>. We can write the neural field equation for <span class="math inline">\(p=1\)</span> in the Gaussian embedding as</p>
<p><span id="eq-nf-gaussian-p1"><span class="math display">\[
\partial_t h(z_1, t) = -h(z_1, t) + \int_{\mathbb R} z_1 \tilde\phi(y_1) \phi(h(y_1, t)) \mathcal{N}(\mathrm dy_1).
\tag{2.4}\]</span></span></p>
</section>
<section id="a-natural-embedding-for-p2" class="level4" data-number="2.1.3.3">
<h4 data-number="2.1.3.3" class="anchored" data-anchor-id="a-natural-embedding-for-p2"><span class="header-section-number">2.1.3.3</span> A natural embedding for <span class="math inline">\(p=2\)</span></h4>
<p>Because <a href="#eq-nf-gaussian-p1">Equation&nbsp;<span>2.4</span></a> already involves a one-dimensional integral, the <span class="math inline">\([0,1]\)</span> embedding can easily be found by applying rescaling methods, such as the inverse CDF method described in <a href="#sec-mapping-cdf"><span>Section&nbsp;2.5</span></a> later in this chapter.</p>
<p>The question of finding a one-dimensional embedding becomes non-trivial when we consider <span class="math inline">\(p&gt;1\)</span>, here we take <span class="math inline">\(p=2\)</span>. In <a href="#fig-p2">Figure&nbsp;<span>2.3</span></a>, we repeat the same simulations as in the previous section. Again, the structure of the network emerges naturally as a smooth surface in <span class="math inline">\(\mathbb R^2\)</span>.</p>
<div id="fig-p2" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><video src="figures/embedding_p=2.mp4" class="img-fluid" width="500" controls=""><a href="figures/embedding_p=2.mp4">Video</a></video></p>
<figcaption class="figure-caption">Figure&nbsp;2.3: Simulation of the Gaussian network of neurons with <span class="math inline">\(p=2\)</span>. Every neuron is represented by a dot, of which the height represents its activity level. Projections are shown by gray points.</figcaption>
</figure>
</div>
<p>We however note that in this case, there are two pattern fixed points that the network can converge to. In the example given here, the network coincidentally converged again to the first pattern fixed point. We see this because the neural field is constant in the <span class="math inline">\(z_2\)</span> direction. We write the fixed point <span class="math inline">\(h^\star(z_1, z_2, t) = z_1\)</span>. The neural field is now written as</p>
<p><span id="eq-nf-gaussian-p2"><span class="math display">\[
\begin{aligned}
\partial_t h(z_1, z_2, t) &amp;= -h(z_1, z_2, t)\\
&amp;+ \int_{\mathbb R} \left(z_1 \tilde\phi(y_1) + z_2 \tilde\phi(y_2)\right) \phi(h(y_1, y_2, t)) \mathcal{N}(\mathrm dy_1) \mathcal{N}(\mathrm dy_2).
\end{aligned}
\tag{2.5}\]</span></span></p>
<p>More generally, we can write the <span class="math inline">\(p\)</span>-dimensional neural field, where <span class="math inline">\(\boldsymbol{z} \in \mathbb R^p\)</span>:</p>
<p><span id="eq-nf-gaussian-p"><span class="math display">\[
\begin{aligned}
\partial_t h(\boldsymbol{z}, t) &amp;= -h(\boldsymbol{z}, t) + \int_{\mathbb R^p} w(\boldsymbol{z}, \boldsymbol{y}) \phi(h(\boldsymbol{y}, t)) \mathcal{N}^p(\mathrm d\boldsymbol{y}) \\
w(\boldsymbol{z}, \boldsymbol{y}) &amp;= \sum_{\mu=1}^p z_\mu \tilde\phi(y_\mu) \\
\mathcal{N}^p(\mathrm d\boldsymbol{y}) &amp;= \frac{\mathrm{exp}(\frac12\sum_{\mu=1}^p y_\mu^2)}{(2\pi)^{p/2}} \mathrm{d} y_1 \cdots \mathrm d y_p.
\end{aligned}
\tag{2.6}\]</span></span></p>
</section>
</section>
</section>
<section id="sec-simnf" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="sec-simnf"><span class="header-section-number">2.2</span> Simulation schemes for neural fields</h2>
<p>Now that we have introduced the analytical neural fields, let us discuss how we can simulate them numerically, more specifically how we can estimate the <span class="math inline">\(p\)</span>-dimensional integral.</p>
<section id="sec-mc" class="level3" data-number="2.2.1">
<h3 data-number="2.2.1" class="anchored" data-anchor-id="sec-mc"><span class="header-section-number">2.2.1</span> Sampling method (Monte Carlo integration)</h3>
<p>Due to the formulation of the neural field weighted by a probability distribution <span class="math inline">\(\rho(y_1, \cdots, y_p)\)</span>, a natural way to estimate the integral is by Monte Carlo integration. The essence of this method is to use the Central Limit Theorem, where we take <span class="math inline">\(N\)</span> independent samples <span class="math inline">\(\boldsymbol{y_i} = (y_{1i}, \cdots, y_{pi})\)</span> from the distribution <span class="math inline">\(\rho\)</span>, and use them to estimate the integrand <span class="math inline">\(I(\boldsymbol{z}, \boldsymbol{y})\)</span>.</p>
<p><span id="eq-mc"><span class="math display">\[
\begin{aligned}
\mathcal{I}(\boldsymbol{z}) &amp;= \int_{\mathbb R^p} \underbrace{w(\boldsymbol{z}, \boldsymbol{y}) \phi(h(\boldsymbol{y}, t))}_{I(\boldsymbol{z}, \boldsymbol{y})} \rho(\mathrm d\boldsymbol{y}) \\
\hat{\mathcal{I}}(\boldsymbol{z}) &amp;= \frac 1N \sum_{i=1}^N I(\boldsymbol{z}, \boldsymbol{y_i}), \, \boldsymbol{y_i} \stackrel{\text{iid}}{\sim} \rho
\end{aligned}
\tag{2.7}\]</span></span></p>
<p>Applying the Central Limit Theorem, we get that the estimation <span class="math inline">\(\mathcal I(\boldsymbol{z})\)</span> from <a href="#eq-mc">Equation&nbsp;<span>2.7</span></a> has a convergence rate of <span class="math inline">\(\mathcal{O}(1/\sqrt{N})\)</span>, since</p>
<p><span class="math display">\[
\begin{aligned}
\mathrm{Var}\left[\hat{\mathcal{I}}(\boldsymbol{z}) \right] &amp;= \frac 1N \mathrm{Var}_{\boldsymbol{y} \sim \rho}[I(\boldsymbol{z}, \boldsymbol{y})].
\end{aligned}
\]</span></p>
<!-- The attentive reader might have noticed by now that the Monte-Carlo method just reduces to simulating the network of neurons @eq-toymodel-evolution. Effectively, the network of neurons converges to the neural field with rate $\bO(1/\sqrt{N})$.

One small nuance we should add, is that strictly speaking, Monte-Carlo integration should resample the integrand at every timestep, but the network of neurons samples the neurons once, and propagates these points through time. -->
</section>
<section id="sec-gridmethod" class="level3" data-number="2.2.2">
<h3 data-number="2.2.2" class="anchored" data-anchor-id="sec-gridmethod"><span class="header-section-number">2.2.2</span> Grid method (Trapezoidal integration)</h3>
<p>Another method to estimate an integral numerically is by discretizing the integrand on a regular grid. We simply state the well-known result that the convergence rate of the estimation is <span class="math inline">\(\mathcal{O}(N^{-2/p})\)</span>. Interestingly, the convergence rate depends on the dimension <span class="math inline">\(p\)</span> of the integral. For <span class="math inline">\(p \geq 4\)</span>, Monte-Carlo integration shows better scaling behaviour (but depending on the use case, prefactors of the error can change which method is better).</p>
<p>In our case, since the distribution <span class="math inline">\(\rho(z_1, \cdots, z_p)\)</span> can be factorized into the product of <span class="math inline">\(p\)</span> independent Gaussian distributions, we can decouple the <span class="math inline">\(p\)</span>-dimensional integral into the product of <span class="math inline">\(p\)</span> one-dimensional integrals. Then the error estimation of the integral scales as <span class="math inline">\(\mathcal{O}(N^{-2})\)</span>. For this reason, we use the grid method for the remainder of this thesis.</p>
</section>
</section>
<section id="characterizing-dynamics-of-the-low-rank-neural-field" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="characterizing-dynamics-of-the-low-rank-neural-field"><span class="header-section-number">2.3</span> Characterizing dynamics of the low-rank neural field</h2>
<p>We now introduce some tools that will help us understand the toy model. We stress that in general, other neural field models might not have this luxury, and that we only make use of these tools to help explain the dynamics of the simulated neural fields. The overlaps and projections introduced here are only computed from the results of the simulation (see <a href="https://github.com/ninivert/lcnthesis/blob/master/notebooks/neurodyn/_overlap.py"><code>overlap.py</code></a> in the source code).</p>
<section id="overlap-variables" class="level3" data-number="2.3.1">
<h3 data-number="2.3.1" class="anchored" data-anchor-id="overlap-variables"><span class="header-section-number">2.3.1</span> Overlap variables</h3>
<p>Overlap variables in continuous time were introduced by <span class="citation" data-cites="Gerstner_1992"><a href="../references.html#ref-Gerstner_1992" role="doc-biblioref">[4]</a></span> to study spiking networks models of associative memory (Hopfield-type networks), which can be seen as a low-rank network with discrete spatial structure instead of our continuous <span class="math inline">\(\boldsymbol{z}\)</span>. The overlap variables measure the correlation between the state of the network (the value of the potential <span class="math inline">\(h(\boldsymbol{z}, t)\)</span> at each point in space) and the pattern fixed point.</p>
<p>For the network for neurons, we write the overlap with the pattern <span class="math inline">\(\mu\)</span> as</p>
<p><span id="eq-overlap"><span class="math display">\[
\begin{aligned}
m_\mu(t) &amp;= \frac 1N \sum_{i=1}^N \tilde \phi(z_{\mu i}) \phi(h_i(t)) \\
&amp;\xrightarrow{N \to \infty} \int_{\mathbb R^p} \tilde \phi(z_\mu) \phi(h(\boldsymbol{z}, t)) \mathcal{N}^p(\mathrm d\boldsymbol{z}).
\end{aligned}
\tag{2.8}\]</span></span></p>
<p>We note that similarly to <span class="citation" data-cites="Gerstner_1992"><a href="../references.html#ref-Gerstner_1992" role="doc-biblioref">[4]</a></span>, we can write the dynamics of the neural field in terms of the overlap variables:</p>
<p><span id="eq-overlap-evolution"><span class="math display">\[
\partial_t h(\boldsymbol{z}, t) = -h(\boldsymbol{z}, t) + \sum_{\mu=1}^p z_\mu m_\mu(t).
\tag{2.9}\]</span></span></p>
<p>This formulation “hides” the integral inside the overlap variables, but one should not forget that the overlaps depend on the neural field at any given time.</p>
</section>
<section id="sec-closed-system" class="level3" data-number="2.3.2">
<h3 data-number="2.3.2" class="anchored" data-anchor-id="sec-closed-system"><span class="header-section-number">2.3.2</span> Low-rank neural fields as <span class="math inline">\(p\)</span>-dimensional closed systems</h3>
<p><a href="#eq-overlap-evolution">Equation&nbsp;<span>2.9</span></a> hints towards the possibility of writing the dynamics of the low-rank system as a closed system of <span class="math inline">\(p\)</span> variables. As explained in <a href="#sec-low-rank-network"><span>Section&nbsp;2.1.1</span></a>, the recurrent currents span a <span class="math inline">\(p\)</span>-dimensional subspace, and the orthogonal component behaves independently. This motivates the formulation of a <span class="math inline">\(p\)</span>-dimensional closed system.</p>
<p>We refer to <span class="citation" data-cites="veltz2009localglobal"><a href="../references.html#ref-veltz2009localglobal" role="doc-biblioref">[5]</a></span>, section 4.3 for the reduction of general low-rank networks to <span class="math inline">\(p\)</span>-dimensional closed systems, and give the equations applied to our toy model. Defining the projections <span class="math inline">\(\kappa_\mu(t)\)</span> of the neural field onto the patterns</p>
<p><span id="eq-kappa"><span class="math display">\[
\kappa_\mu(t) = \int_{\mathbb R^p} y_\mu h(\boldsymbol{y}, t) \mathcal{N}^p(\mathrm d\boldsymbol{y}),
\tag{2.10}\]</span></span></p>
<p>we can decompose the neural field onto the basis of patterns (mathematically, the patterns form an orthonormal basis of functions):</p>
<p><span class="math display">\[
h(\boldsymbol{z}, t) = h^\perp(\boldsymbol{z}, t) + \sum_{\mu=1}^p \kappa_\mu(t) z_\mu.
\]</span></p>
<p>The orthonormal component <span class="math inline">\(h^\perp(\boldsymbol{z}, t)\)</span> is independent of the rest of the system and decays exponentially. Then, the equations of evolution for the projections are given by the following closed system:</p>
<p><span class="math display">\[
\begin{aligned}
\dot \kappa_\mu(t) &amp;= -\kappa_\mu(t) + \int_{\mathbb R^p} \tilde\phi(y_\mu) \phi(h(\boldsymbol{y}, t)) \mathcal{N}^p(\mathrm d \boldsymbol{y}) = -\kappa_\mu(t) + m_\mu(t) \\
\kappa_\mu(0) &amp;= \int_{\mathbb{R}^p} y_\mu h(\boldsymbol{y}, 0) \mathcal{N}^p(\mathrm d \boldsymbol{y}),
\end{aligned}
\]</span></p>
<p>and the orthogonal component evolves according to</p>
<p><span class="math display">\[
\begin{aligned}
h^\perp(\boldsymbol{z}, t) &amp;= h^\perp(\boldsymbol{z}, 0) \mathrm e^{-t}\\
h^\perp(\boldsymbol{z}, 0) &amp;= h(\boldsymbol{z}, 0) - \sum_{\mu=1}^p \kappa_\mu(0) z_\mu.
\end{aligned}
\]</span></p>
<p>The set of <span class="math inline">\(\kappa_\mu(t)\)</span> defines a trajectory in a <span class="math inline">\(p\)</span>-dimensional <em>latent space</em>. We note that similarly to <a href="#eq-overlap-evolution">Equation&nbsp;<span>2.9</span></a>, the overlaps intervene in the equations of evolution, and carry the information on the neural field.</p>
<p>Additionally, it can be seen from the expression of the overlaps in <a href="#eq-overlap">Equation&nbsp;<span>2.8</span></a>, that in the case of a linear activation function <span class="math inline">\(\phi(h) = c_0 + c_1 h\)</span>, we have the equality <span class="math inline">\(m_\mu(t) = \kappa_\mu(t)\)</span>.</p>
</section>
</section>
<section id="sec-cycling-nf" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="sec-cycling-nf"><span class="header-section-number">2.4</span> A cycling neural field</h2>
<p>In <a href="#sec-networks-of-neurons-to-neural-fields"><span>Section&nbsp;2.1</span></a> we introduced a simple toy model of a neural field with a natural embedding on the <span class="math inline">\(p\)</span>-dimensional Gaussian. Analytical derivations accompanied by numerical simulations showed the dynamics of this model can be summarized by the convergence to pattern fixed points. In this section, we modify the toy model minimally, such that we can observe more interesting behaviour in the form of cycling.</p>
<p>We modify <a href="#eq-nf-gaussian-p">Equation&nbsp;<span>2.6</span></a> in two ways:</p>
<ol type="1">
<li>We define a time delay <span class="math inline">\(\delta\)</span> with which a neuron will “wait” before reacting to a change in its potential.</li>
<li>We define “rolling” as the response of a neuron to shift towards the “next” pattern given its state matching a current pattern. This is done by adding a shift <span class="math inline">\(\mu+1\)</span> in the connectivity kernel, with the convention <span class="math inline">\(p+1=1\)</span> (see <span class="citation" data-cites="Ami89"><a href="../references.html#ref-Ami89" role="doc-biblioref">[6]</a></span>, Chapter 5).</li>
</ol>
<p>The resulting cycling neural field is written as such:</p>
<p><span id="eq-nf-cycling-p"><span class="math display">\[
\partial_t h(\boldsymbol{z}, t) = -h(\boldsymbol{z}, t) + \sum_{\mu=1}^p \int_{\mathbb{R}^p} z_{\mu+1} \tilde \phi (y_\mu) \phi(h(\boldsymbol{y}, t - \delta)) \mathcal{N}^p(\mathrm d \boldsymbol{y})
\tag{2.11}\]</span></span></p>
<p>The geometric intuition behind this formulation is that now the recurrent currents are “rotated by half a turn” (around the axis normal to the plane <span class="math inline">\((z_\mu, z_{\mu+1})\)</span>) in the embedding. When the network is at a pattern fixed point <span class="math inline">\(\mu\)</span>, the recurrent drive will then push it towards the pattern <span class="math inline">\(\mu+1\)</span>. <a href="#fig-cycling-irec">Figure&nbsp;<span>2.4</span></a> shows the difference in the behaviour of the original and the cycling neural field.</p>
<div class="cell" data-execution_count="5">
<div class="cell-output cell-output-display">
<div id="fig-cycling-irec" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="2_neural_fields_files/figure-html/fig-cycling-irec-output-1.svg" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;2.4: Intuition for the cycling behavior. We consider the network in the state <span class="math inline">\(h(z_1,z_2)=z_1\)</span>, and look at the recurrent currents. The recurrent currents align with the activity for the original neural field, and the effective change in potential is zero everywhere. For the cycling neural field, the recurrent currents are orthogonal to the activity, such that with the decay term, the effective change leads to a rotation in the counter-clockwise direction, towards the state <span class="math inline">\(h(z_1,z_2)=z_2\)</span>.</figcaption>
</figure>
</div>
</div>
</div>
<p>We note however that <a href="#eq-nf-cycling-p">Equation&nbsp;<span>2.11</span></a> no longer is a partial differential equation, but rather a delayed differential equation, which are in general much more complicated to solve. We make a simplifying assumption to extend the initial condition back in time, such that <span class="math inline">\(h(\boldsymbol{z}, t &lt; 0) = h(\boldsymbol{z}, 0)\)</span> is a constant function. Numerically, the delayed partial differential equation also means that we have to store a history of the neural fields instead of only the current one. Since the <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.solve_ivp.html"><code>solve_ivp</code></a> method dynamically adapts the timestep of integration, we use linear interpolation between two known points in the stored neural field states to estimate <span class="math inline">\(h(\boldsymbol{z}, t-\delta)\)</span> (see <a href="https://github.com/ninivert/lcnthesis/blob/master/notebooks/neurodyn/_lagging.py"><code>lagging.py</code></a> and <a href="https://github.com/ninivert/lcnthesis/blob/master/notebooks/neurodyn/_rnn.py#L334">its use</a>).</p>
<p>With these considerations, we simulate a <span class="math inline">\(p=2\)</span> cycling neural field with <span class="math inline">\(\delta=6\)</span> (since the membrane time constant has been fixed to one, this means that <span class="math inline">\(\delta = 6\tau = 6\)</span>) and initial condition <span class="math inline">\(h(z_1, z_2, t &lt; 0) = z_1\)</span> in <a href="#fig-cycling-anim">Figure&nbsp;<span>2.5</span></a>. The latent trajectory is estimated from the simulation results, and animated simultaneously with the neural field. We clearly see the oscillations between the patterns <span class="math inline">\(\mu=1\)</span> and <span class="math inline">\(\mu=2\)</span>.</p>
<div id="fig-cycling-anim" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><video src="figures/embedding_p=2_cycling.mp4" class="img-fluid" controls=""><a href="figures/embedding_p=2_cycling.mp4">Video</a></video></p>
<figcaption class="figure-caption">Figure&nbsp;2.5: A cycling neural field with <span class="math inline">\(\delta=6\)</span> and initial condition <span class="math inline">\(h(z_1, z_2, t &lt; 0) = z_1\)</span>. The latent trajectory is also computed, and demonstrates the oscillatory behaviour.</figcaption>
</figure>
</div>
<p>Relating to <a href="#sec-closed-system"><span>Section&nbsp;2.3.2</span></a>, the dynamics of the projection now read:</p>
<p><span class="math display">\[
\dot \kappa_\mu(t) = -\kappa_\mu(t) + m_{\mu+1}(t - \delta).
\]</span></p>
<!-- This system of equations can be solved by steps of $\delta$, iteratively plugging in the previous solution to solve each cycle. -->
<!-- When the neural field starts in the pattern $\nu$, that is $h(\vec z, 0) = z_\nu$, this translates to $\kappa_\nu(t \leq \delta) = 1$, and all other $\kappa_\mu(t \leq \delta) = 0,\,\mu\neq\nu$. Additionally, let us suppose that $\phi$ is linear, so that $m_\mu(t) = \kappa_\mu(t)$, and let us take $p=2$. Then we can solve the equation for the "first cycle" $0 < t \leq \delta$:

$$
\begin{aligned}
\kappa_1(t) &= \mathrm{e}^{-t} \\
\kappa_2(t) &= 1 - \mathrm{e}^{-t}.
\end{aligned}
$$

With this, we see that the projections "alternate" back and forth with period $\delta$. -->
</section>
<section id="sec-mapping-cdf" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="sec-mapping-cdf"><span class="header-section-number">2.5</span> Mapping <span class="math inline">\(\mathbb R^2\)</span> to <span class="math inline">\([0,1]^2\)</span> using the CDF</h2>
<p>In preparation to the next chapter, we first map the 2-dimensional neural field equation in <span class="math inline">\(\mathbb R^2\)</span> to <span class="math inline">\([0,1]^2\)</span>, and, in general <span class="math inline">\(\mathbb R^p\)</span> to <span class="math inline">\([0,1]^p\)</span>. There are many ways to do this by using functions <span class="math inline">\(\mathbb R\mapsto [0,1]\)</span>, but a practical choice is the Gaussian CDF (Cumulative Density Function), because it has the benefit of absorbing the density into the kernel, in the sense that the integral <span class="math inline">\([0,1]^p\)</span> becomes weighted by the uniform distribution. The one-dimensional Gaussian CDF is defined as</p>
<p><span class="math display">\[
\mathrm{CDF}(z) = \int_{-\infty}^z \mathcal{N}(\mathrm dy)
\]</span></p>
<p>and maps <span class="math inline">\(\mathbb R\)</span> to <span class="math inline">\([0,1]\)</span>,<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> as pictured in <a href="#fig-gaussian-cdf">Figure&nbsp;<span>2.6</span></a>.</p>
<div class="cell" data-execution_count="7">
<div class="cell-output cell-output-display">
<div id="fig-gaussian-cdf" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="2_neural_fields_files/figure-html/fig-gaussian-cdf-output-1.svg" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;2.6: CDF of the Gaussian distribution</figcaption>
</figure>
</div>
</div>
</div>
<p>Defining the change of variables <span class="math inline">\(v_\mu=\mathrm{CDF}(z_\mu), u_\mu=\mathrm{CDF}(y_\mu)\)</span>, we can define the neural field <span class="math inline">\(h_U\)</span> and the connectivity kernel <span class="math inline">\(w_U\)</span> for the uniform distribution as</p>
<p><span id="eq-hu-wu"><span class="math display">\[
\begin{aligned}
h_U(\boldsymbol{v}, t) &amp;= h(\mathrm{CDF}^{-1}(\boldsymbol{v}), t) = h(\mathrm{CDF}^{-1}(v_1), \cdots, \mathrm{CDF}^{-1}(v_p)) \\
w_U(\boldsymbol{v}, \boldsymbol{u}) &amp;= w(\mathrm{CDF}^{-1}(\boldsymbol{v}), \mathrm{CDF}^{-1}(\boldsymbol{u})).
\end{aligned}
\tag{2.12}\]</span></span></p>
<p>After the change of variables, the neural field in <span class="math inline">\([0,1]^p\)</span> is equivalent to the neural field in <span class="math inline">\(\mathbb R^p\)</span>, and its equation reads</p>
<p><span class="math display">\[
\partial_t h_U(t, \boldsymbol{v}) = -h_U(t, \boldsymbol{v}) + \int_{[0,1]^p} w_U(\boldsymbol{v}, \boldsymbol{u}) \phi(h_U(t, \boldsymbol{u})) \mathrm d \boldsymbol{u}.
\]</span></p>
<p>Defining the neural field in <span class="math inline">\([0,1]^p\)</span> also addresses the problem that we tried to hide in <a href="#sec-gridmethod"><span>Section&nbsp;2.2.2</span></a> relating to the difficulties of defining a grid on <span class="math inline">\(\mathbb R^p\)</span>. With the integral on <span class="math inline">\([0,1]^p\)</span>, we can simply use a uniform grid, which maps back to samples in <span class="math inline">\(\mathbb R^p\)</span> by using the inverse CDF, as shown in <a href="#fig-grid-correspondance">Figure&nbsp;<span>2.7</span></a>.</p>
<div class="cell" data-execution_count="8">
<div class="cell-output cell-output-display">
<div id="fig-grid-correspondance" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="2_neural_fields_files/figure-html/fig-grid-correspondance-output-1.svg" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;2.7: A grid in <span class="math inline">\([0,1]^2\)</span> and the corresponding samples in <span class="math inline">\(\mathbb R^2\)</span>. The numeric density of points in <span class="math inline">\(\mathbb R^2\)</span> approaches the normal distribution as the grid becomes finer (here, the mesh size is <span class="math inline">\(2^{-4}\)</span> along each dimension).</figcaption>
</figure>
</div>
</div>
</div>
<p>We make the final remark that this method of mapping <span class="math inline">\(\mathbb R^p\)</span> to <span class="math inline">\([0,1]^p\)</span> by using the CDF does not work for any neural field. In our case, we use the fact that the distribution factorizes into <span class="math inline">\(\rho(z_1, \cdots, z_p) = \rho_1(z_1) \cdots \rho_p(z_p)\)</span>, such that the components of <span class="math inline">\(\boldsymbol{v}\)</span> are independent. We can then define the “componentwise CDF” that relates to the total CDF via</p>
<p><span class="math display">\[
\begin{aligned}
\mathrm{CDF}(z_1, \cdots, z_p) &amp;= \int_{-\infty}^{z_1} \cdots \int_{-\infty}^{z_p} \rho(\mathrm dy_1, \cdots, \mathrm dy_p) \\
&amp;= \int_{-\infty}^{z_1} \rho_1(\mathrm dy_1) \cdots \int_{-\infty}^{z_p} \rho_p(\mathrm dy_p) \\
&amp;= \mathrm{CDF}(z_1) \cdots \mathrm{CDF}(z_p) \\
&amp;= v_1 \cdots v_p = \prod_{\mu=1}^p v_\mu.
\end{aligned}
\]</span></p>


<div id="refs" class="references csl-bib-body" role="list" style="display: none">
<div id="ref-JazOst21" class="csl-entry" role="listitem">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline">M. Jazayeri and S. Ostojic, <span>“Interpreting neural computations by examining intrinsic and embedding dimensionality of neural activity,”</span> <em>Current opinion in neurobiology</em>, vol. 70, pp. 113–120, 2021. </div>
</div>
<div id="ref-schmutz2023convergence" class="csl-entry" role="listitem">
<div class="csl-left-margin">[2] </div><div class="csl-right-inline">V. Schmutz, J. Brea, and W. Gerstner, <span>“Convergence of redundancy-free spiking neural networks to rate networks,”</span> 2023 [Online]. Available: <a href="https://arxiv.org/abs/2303.05174">https://arxiv.org/abs/2303.05174</a></div>
</div>
<div id="ref-Beiran2021" class="csl-entry" role="listitem">
<div class="csl-left-margin">[3] </div><div class="csl-right-inline">M. Beiran, A. Dubreuil, A. Valente, F. Mastrogiuseppe, and S. Ostojic, <span>“Shaping dynamics with multiple populations in <span>Low-Rank</span> recurrent networks,”</span> <em>Neural Comput</em>, vol. 33, no. 6, pp. 1572–1615, May 2021. </div>
</div>
<div id="ref-Gerstner_1992" class="csl-entry" role="listitem">
<div class="csl-left-margin">[4] </div><div class="csl-right-inline">W. Gerstner and J. L. van Hemmen, <span>“Associative memory in a network of ’spiking’ neurons,”</span> <em>Network: Computation in Neural Systems</em>, vol. 3, no. 2, p. 139, May 1992, doi: <a href="https://doi.org/10.1088/0954-898X/3/2/004">10.1088/0954-898X/3/2/004</a>. [Online]. Available: <a href="https://dx.doi.org/10.1088/0954-898X/3/2/004">https://dx.doi.org/10.1088/0954-898X/3/2/004</a></div>
</div>
<div id="ref-veltz2009localglobal" class="csl-entry" role="listitem">
<div class="csl-left-margin">[5] </div><div class="csl-right-inline">R. Veltz and O. Faugeras, <span>“Local/global analysis of the stationary solutions of some neural field equations.”</span> 2009 [Online]. Available: <a href="https://arxiv.org/abs/0910.2247">https://arxiv.org/abs/0910.2247</a></div>
</div>
<div id="ref-Ami89" class="csl-entry" role="listitem">
<div class="csl-left-margin">[6] </div><div class="csl-right-inline">D. J. Amit, <em>Modeling brain function: The world of attractor neural networks</em>. Cambridge university press, 1989. </div>
</div>
</div>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>In the source code, implementation for this can be found under <code>notebooks/neurodyn/_rnn.py</code>, in the method <a href="https://github.com/ninivert/lcnthesis/blob/master/notebooks/neurodyn/_rnn.py#L154"><code>LowRankRNNParams.new_sampled_valentin</code></a>.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>See the implementation in <code>notebooks/neurodyn/_rnn.py</code>, in the method <a href="https://github.com/ninivert/lcnthesis/blob/master/notebooks/neurodyn/_rnn.py#L302"><code>LowRankRNN.I_rec</code></a>.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Strictly speaking the image of <span class="math inline">\(\mathbb R\)</span> is in <span class="math inline">\((0,1)\)</span>, but for simplicity we consider <span class="math inline">\(\mathbb R\cup \{-\infty, +\infty\}\)</span> which maps to <span class="math inline">\([0,1]\)</span>. This is not fundamentally important, because numerically the infinities are never touched, and analytically the recurrent currents given by the integration vanish at infinity due to the Gaussian weighting.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../chapters/1_intro.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../chapters/3_mappings.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Mapping neural fields to lower-dimensional embeddings</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>