# Neural field toy model and simulations {#sec-nf}

::: {.content-hidden}
{{< include ../macros.tex >}}
:::

In this chapter, we introduce a toy model so that we can later study the mappings of embedding spaces. We stress that this toy model serves only for illustration purposes, and our results apply to any neural field. For this reason, we only cite or give short derivations of results used to understand the model, and the influence of the mappings that are later applied to it.

## Networks of neurons to neural fields

### Low-rank networks of neurons

The toy model we introduce is an instance of a low-rank rate neural network. Such models hold their name from the form of their connectivity matrix, which has a rank of (at most) $p$. We write the connectivity matrix as

$$
J_{ij} = \frac 1N \sum_{\mu=1}^p F_{\mu i} G_{\mu j}.
$$ {#eq-lowrankj}

The geometric view of these networks is that the recurrent currents lie in a $p$-dimensional subspace of $\R^N$ spanned by the vectors $\{\vec{F_1}, \cdots, \vec{F_p}\}$ (where $\vec{F_\mu}=(F_{\mu 1}, \cdots F_{\mu N})$), which therefore form a "natural" embedding.

Recalling the notation from the introduction, $h_i(t)$ are the neuron potentials (with initial condition $h_i(0)$), $\phi : \R \mapsto \R^+$ is the monotonic increasing activation function, and we write the evolution equation for this network of neurons:

$$
\dot h_i(t) = \underbrace{-h_i(t)}_\text{exponential decay} + \underbrace{\frac 1N \sum_{\mu=1}^p \sum_{j=1}^{N} F_{\mu i} G_{\mu j} \phi(h_j(t))}_\text{recurrent current $I^\text{rec}_i(t)$}.
$$ {#eq-lowrank-evolution}

The exponential decay term describes how, in isolation, a neuron's potential tends to zero, biologically corresponding to a depolarized membrane. The recurrent current term $I^\text{rec}_i(t)$ describes the "currents" received by neuron $i$ from all the other neurons in the network.

We would like to add a few remarks on the physical units of @eq-lowrank-evolution. Strictly speaking, a prefactor $\tau$ with units of $\mathrm{seconds}^{-1}$ should be multiplied with $\dot h_i(t)$ of units $\mathrm{volt}\cdot\mathrm{seconds}^{-1}$, such that the left hand side of the equation is consistent with the unit of voltage. Similarly, the activation function has units $\mathrm{seconds}^{-1}$ and the connectivity matrix units of $\mathrm{coulomb}$, and a multiplicative term $R$ with units $\mathrm{ohm}$ should multiply the recurrent currents in order to yield a voltage. By rescaling of time or connection weights both $\tau$ and $R$ can be set to one, and so we omit to write them.

One last remark is that in our model, we omit to add any source of external currents: from the initial condition, the network is left to evolve in isolation.

### Gaussian $\mathcal{N}(\vec 0, \mathbb{1}_N)$ low-rank network of neurons

Let us now specify the low-rank model introduced in the previous section, and use the model introduced in @schmutz2023convergence. This paper defines the low-rank vectors $F_{\mu i}$ such that each component independantly samples a standard gaussian (zero mean, unit variance). In other words, every neuron $i$ samples a vector $\vec{F_i} = (F_{1i}, \cdots, F_{pi})$ from the $p$-dimensional gaussian ball. We write

$$
\vec{F_i} = \vec{z_i},\ \vec{z_i} \stackrel{\text{iid}}{\sim} \rho(z_1, \cdots, z_p),
$$

where

$$
\begin{aligned}
\rho(z_1, \cdots, z_p) &= \prod_{\mu=1}^p \mathcal{N}(z_\mu) \\
\mathcal{N}(z) &= \frac{1}{\sqrt{2 \pi}} \mathrm{e}^{-\frac 12 z^2}.
\end{aligned}
$$

The vectors $G_{\mu i}$ are defined as

$$
G_{\mu i} = \tilde\phi(z_{i \mu}) \stackrel{\text{def}}{=} \frac{\phi(z_{i \mu}) - \avg{z_{i \mu}}}{\mathrm{Var}[\phi(z_{i \mu})]}.
$$

This choice be elucidated in a few paragraphs. Finally, the activation function remains to be defined. For simplicity, we take it to be the logistic function, although the following results are not sensitive to this choice.

$$
\phi(h) = \frac{1}{1 + \mathrm{e}^h}
$$

Putting everything together, we get the following expression for the equation of evolution of the network of neurons:

$$
\dot h_i(t) = -h_i(t) + \frac 1N \sum_{\mu=1}^p \sum_{j=1}^{N} z_{\mu i} \tilde \phi(z_{\mu j}) \phi(h_j(t)).
$$ {#eq-toymodel-evolution}

We note that contrary to the model introduced in @schmutz2023convergence, we ignore the self-connections term, because it introduces a vanishing correction of order $\bO(1/N)$.

In order to understand this model better, we can do an analysis of its fixed points and their stability. Here we summarize the results from the derivation presented in @sec-fixedpoints. We use the notation $h^\star$ to refer to fixed points of the network. The fixed points solve the roots of the evolution equation:

$$
\dot h_i(t)\ \text{evaluated at}\ h_i^\star = -h^\star_i(t) + \frac 1N \sum_{\mu=1}^p \sum_{j=1}^{N} z_{\mu i} \tilde \phi(z_{\mu j}) \phi(h^\star_j(t)) = 0
$$

* $h^\star_i = z_{\mu i}$ is a stable fixed point for all $\mu \in \{1,\cdots,p\}$. Additionally, because the Gaussian distribution is symmetric around zero, $h_i = -z_{\mu i}$ are also stable fixed points. We refer to these fixed points as the "pattern" fixed points, because the network of neurons "remembers" the vectors $\vec{F_1}, \cdots, \vec{F_p}$.
* $h^\star_i=0$ is an unstable fixed point. It corresponds to all the neuron potentials being set to zero.

The analysis of stability from @sec-fixedpoints-stability can be summarized by the study of the eigenvalues of the matrix $K_{ij} = J_{ij} \partial \phi(h^\star_j) - \mathrm{Id}_{ij}$.

1. Taking Taylor expansions of the activation functions reveals that -- at least up to order 3 -- uneven powers tend to increase stability of the pattern fixed points (and reciprocally, decrease stability of the zero fixed point), and vice-versa for the even powers (with the notable exception of the constant offset, which doesn't play a role in this analysis).
2. A steeper slope at the origin of the activation function also seems to improve stability of the pattern fixed points. This corresponds to a sharper difference between the "inactive" (low potential) and the "active" (high potential) neurons. The intuition behind this is that if the slope is stronger, then the bump of $\partial \phi$ becomes "more concentrated", which helps lower the eigenvalues associated to the pattern fixed points.
3. The spectrum of $K$ is composed of $p$ eigenvalues that depend on the fixed point, and $N-p$ eigenvalues $\lambda_\perp = -1$ corresponding to the orthogonal complement of the pattern fixed points.

In our case, this analysis in Taylor expansions was sufficient to explain the observed stability resulting from a logistic activation function.

We should however note that the results obtained in @sec-fixedpoints are valid in the $N \to \infty$ limit. A numerical analysis shows in @fig-spectrum that already at $N=64$, the analytic eigenvalues match the numerical eigenvalues closely, and this correspondance improves when we take larger $N$ (typically we will for the rest of this Thesis take $N > 10^3$).

![(TODO add spectrum for zero fixed point) Spectrum of $K$ for $N=64$ for the pattern fixed point](figures/spectrum_N=64.png){#fig-spectrum}

### Emergence of structure in the networks of neurons

show simulations, first in $p=1$, then $p=2$.

![simulation in 1D, p=1](figures/embedding_d=1_h0=0_phi=sigmoid.mp4)

![simulation in 2D, p=2. TODO redo this one with only scatter and latent-](figures/embedding_d=2_h0=0.mp4)

show it converges to the fixed points $z_1$ and $z_2$

the structure of the scatterplot emerges naturally as a smooth manifold in $\R^2$

introduce the neural field as a "smooth fitting of the scatterplot"

eq of neural field

## Simulation schemes for neural fields {#sec-simnf}

we use the RK4 integration scheme, and the integral is decoupled into product of 1D integrals, each is estimated using a dot product

### Sampling method (Monte-Carlo integration)

Neurons are at position $\vec z = (z_1, \cdots, z_p) \in \mathbb{R}^p$, distributed according to the distribution $\frac{\mathrm{exp}(\frac12\sum_{\mu=1}^p z_\mu^2)}{(2\pi)^{p/2}} \mathrm{d} z_1 \cdots \mathrm d z_p = \rho(\mathrm d \vec z)$

The RNN potential becomes $h(t, \vec z)$ and evolves according to

$$
\begin{aligned}
\partial_t h(t, \vec z) &= -h(t, \vec z) + \int_{\Rp} w(\vec z, \vec y) \phi(h(t, \vec y)) \rho(\mathrm d \vec y),
\quad
w(\vec z, \vec y) = \sum_{\mu=1}^p \tilde \phi (y_\mu) z_\mu
\end{aligned}
$$

repeat the fixed points are now the generalization, functions $h=z_1$ and $h=z_2$

simulation : when we simulate a network of neurons with $N \gg 1$, we are approximating the integral using a monte carlo method.

Monte-carlo estimation of the integral is

$$
\begin{aligned}
\mathcal{I}(\vec z) &= \int_{\Rp} \underbrace{w(\vec z, \vec y) \phi(h(t, \vec y))}_{I(\vec z, \vec y)} \rho(\d \vec y) \\
\hat{\mathcal{I}}(\vec z) &= \frac 1N \sum_{i=1}^N I(\vec z, \vec{y_i}), \, \vec{y_i} \stackrel{\text{i.i.d.}}{\sim} \rho
\end{aligned}
$$

Numerically, we are estimating $\hat{\mathcal{I}}(\vec z)$, and we can show the convergence rate is in $\bO(1/\sqrt{N})$, since

$$
\begin{aligned}
\mathrm{Var}\left[\hat{\mathcal{I}}(\vec z) \right] &= \frac 1N \mathrm{Var}_{\vec y \sim \rho}[I(\vec z, \vec y)]
\end{aligned}
$$

Detail : strictly speaking, MC samples new points every timestep, here we sample once at t=0, then propagate these points

### Grid method (Trapezoidal integration) {#sec-gridmethod}

This method describes the trapezoidal method of integration. We state the result that the convergence rate is $\bO(N^{-2/p})$. For $p \geq 4$, MC shows better scaling behavior (but prefactors of error can change which method is better).

In our case, since the distribution $\rho$ can be factorized, we can decouple the $p$-dimensional integral into the product of $p$ one-dimensional integrals. Then the error estimation of the integral scales as $\bO(N^{-2})$. For this reason, we use the grid method for the remainder of this thesis.

## Characterizing dynamics of the low-rank neural field

### Overlap variables

networks of neurons : 

$$
m_\mu(t) = \frac 1N \sum_{i=1}^N \tilde \phi(\xi_{\mu,i}) \phi(h_i(t))
$$

neural field :

$$
m_\mu(t) = \int_{\mathbb R^p} \tilde \phi(y_\mu) \phi(h(t,\vec y)) \rho(\mathrm d \vec y)
$$

note that :

$$
\partial_t h(t, \vec z) = -h(t, \vec z) + \sum_{\mu=1}^p z_\mu m_\mu(t)
$$

[TODO : i'm not even sure that introducing the overlaps in necessary. we might just include them for completeness, and because they are relevant in papers on low-rank stuff. in the rest, just use the kappa for the latent space]

[TODO : do not expand mathematically, say this is well-known and given for context. the fixed point and stability study is new though]

### Low-rank neural fields as $p$-dimensional closed systems

cite @veltz2009localglobal for this.

on explicite les equations du paper dans notre cas

introduce this a just the projection of the field on the fixed points

introduce kappa, and in our case the low-rank model the kappa

we define

$$
\kappa_\mu(t) = \avg{f_\mu, h(t, \cdot)} = \int_{\Rp} y_\mu h(0, \vec y) \rho(\mathrm d \vec y)
$$

with these definitions, we have in our case
$$
\begin{aligned}
\dot \kappa_\mu(t) &= -\kappa_\mu(t) + \int_{\Rp} \tilde\phi(y_\mu) \phi(h(t, \vec y)) \rho(\mathrm d \vec y) = -\kappa_\mu(t) + m_\mu(t) \\
\kappa_\mu(0) &= \int_{\mathbb{R}^p} y_\mu h(0, \vec y) \rho(\mathrm d \vec y)
\end{aligned}
$$

and the orthogonal component evolves according to

$$
h^\perp(t, \vec z) = h^\perp(0, \vec z) \mathrm e^{-t},
\quad
h^\perp(0, \vec z) = h(0, \vec z) - \sum_{\mu=1}^p \kappa_\mu(0) z_\mu
$$

todo : put accent on the fact the RHS can be computed using all the kappas, kappa -> h -> integral = m -> RHS

conceptually : do not forget that the kappa is only relevant in this particular toy model, in general we do not have these kappa, and the study of low-dim fields is interesting. If we always had the kappa, then of course we can only look at the kappa, since much simpler (a 1D manifold (line) in a Rp space, instead of a Rp volume in a Rp space).

warning : we need absolutely to distinguish the latent and the embedding space

<!-- Relation between $p$-dimensional closed system and neural field equation in $\mathbb R^p$
$$
\begin{aligned}
\partial_t h(t, \vec z) &= \partial_t h^\perp(t, \vec z) + \sum_{\mu=1}^p z_\mu \dot \kappa_\mu(t) \\
&= -h^\perp(t, \vec z) - \sum_{\mu=1}^p z_\mu \kappa_\mu(t) + \sum_{\mu=1}^p z_\mu m_\mu(t) \\
&= -h(t, \vec z) + \sum_{\mu=1}^p z_\mu m_\mu(t)
\end{aligned}
$$ -->

overlaps as an approximation of the “dynamics” (latent space), in the case $\phi=\mathrm{linear}$. since in our case $\phi$ is a sigmoid, it can be considered linear in the range $h \sim \pm 3$. we therefore use the computed overlaps as to show the latent space

we use the latent space to assert dynamics are the same

<!-- [TODO FIG : draw a flow field of in the latent space. -> actually, might be difficult when there is delay] -->

## A cycling neural field

### Intuition behind the cycling

why ? dynamics are more interesting

we introduce $\delta$ the delay, and rolling $\mu+1$, with convention $p+1=1$

$$
\partial_t h(t, \vec z) = -h(t, \vec z) + \int_{\mathbb{R}^p} \sum_{\mu=1}^p \tilde \phi (y_\mu) z_{\mu+1} \phi(h(t - \delta, \vec y)) \rho(\mathrm d \vec y)
$$

![Intuition for the cycling behavior by plotting the recurrent currents](figures/cycling_recurrent.png)

note this is now a DDE instead of a PDE

![](figures/embedding_d=2_h0=0_cycling_delta=6.mp4)

### Cycling behavior of the latent variables

1. delayed neural field
2. rolled neural field

[TODO : is this useful ? I derived this because it shows we need both delay and rolling, but might be too much]

we can show that in this case

$$
\dot \kappa_\mu(t) = -\kappa_\mu(t) + m_{\mu+1}(t - \delta)
$$

then in the case that $\phi=linear \implies \kappa_\mu = m_\mu$, and initial conditions $m_\mu(t<0) = m_\mu(0)$ (constant inititial condition), $m_1(0) = 1$ and $m_2(0) = 0$, we can solve the first "cycle" ($0 < t < \delta$) analytically :

$$
\begin{aligned}
\kappa_1(t) &= \mathrm{e}^{-t} \\
\kappa_2(t) &= 1 - \mathrm{e}^{-t} \\
\end{aligned}
$$

then one can repeat this for the next cycles. basically each cycle is applying an exponential convolution filter [TODO : trim this, don't go into detail]

<!-- [TODO MATH : the derivation of this, we have an exponential convolution filter] -->

![simulate cycling in p=2](figures/kappa_cycling_p=2.tmp.png)

## Mapping $\R^2$ to $[0,1]^2$ using the CDF {#sec-mapping-cdf}

<!-- ### Using the CDF -->

later we will map [0,1]² -> [0,1], so we first want to map R² -> [0,1]².

there are many ways to do this using functions R -> [0,1], but a practical choice is the inverse CDF, which has the added benefit of absorbing the density in the kernel, in the sense that the integral over [0,1]² becomes "weighted" by a uniform distribution (lesbesgue measure $\lambda$)

Defining the change of variables $u_\mu=\mathrm{CDF}(y_\mu), v_\mu=\mathrm{CDF}(z_\mu)$, the neural field becomes

$$
\begin{aligned}
& \partial_t h_U(t, \vec v) = -h_U(t, \vec v) + \int_{[0,1]^p} w_U(\vec v, \vec u) \phi(h_U(t, \vec u)) \mathrm d \vec u \\
& \text{where } w_U(\vec v, \vec u) = w(\mathrm{CDF}^{-1}(\vec v), \mathrm{CDF}^{-1}(\vec u)), \, h_U(t, \vec v) = h(t, \mathrm{CDF}^{-1}(\vec v))
\end{aligned}
$$

![normal distribution CDF](figures/1920px-Normal_Distribution_CDF.svg.png)

We now have a neural field equation on a unit (hyper)cube, and uniform sampling.

the problem of sampling all of $\R^2$ is now "hidden away" in the CDF, since it goes to $\pm \infty$ at 0 and 1.

note : here we use the fact that the components are independant. therefore we can define the total as the product, i.e.

$$
\mathrm{CDF}(z_1, z_2) = \int_{-\infty}^{z_1} \rho_1(d y_1) \int_{-\infty}^{z_2} \rho_2(\d y_2) = v_1 v_2
$$

we would not be able to do this is the coordinates were coupled, then we could define still define the CDF, but we cannot factorize it into 2 different components

![samples in $\Rp$ and corresponding samples in $[0,1]$](figures/sampling_R2_01.png)

give a picture of the "isolines" of the 2D gaussian CDF

furthermore numerically, mapping R² -> [0,1]² adresses the problem of "putting a finite grid on the infinite R²". (previously we would have had to make the approximation of compact support, then sample the PDF on the grid and numerically renormalize.)

<!-- ### Consequences on simulations

#### Direct sampling

we simulate the network of neurons

#### Grid over $[0,1]^2$

We define $\vec z(\alpha) = S(\alpha)$ the 2D point corresponding to the mapping $\alpha$. (details : we need a bounding box to map back). Defining a matrix $Z_{\mu,\alpha} = \vec z(\alpha)_\mu$, we can write down a numerical PDF $\tilde \rho(Z_{1,\alpha},\cdots,Z_{p,\alpha}) = \tilde \rho(Z_{:,\alpha}) = \frac{\rho(Z_{:,\alpha})}{\sum_\beta \rho(Z_{:,\beta})}$ and the following patterns to simulate the embedded $[0,1]$ neural field as a low-rank RNN.

$$
\tilde F_{\mu,\alpha} = Z_{\mu,\alpha}, \quad \tilde G_{\mu,\alpha} = \tilde\phi(Z_{\mu,\alpha}), \quad \tilde J_{\alpha,\beta}=\tilde \rho(Z_{:,\beta}) \sum_{\mu=1}^p \tilde F_{\mu,\alpha} \tilde G_{\mu,\beta}
$$

Doing this is equivalent, but more memory-efficient to the formulation of the binned connectivity matrix.

![](figures/sampling_R2_01_kernel.png) -->