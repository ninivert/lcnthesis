# Neural field toy model and simulations {#sec-nf}

::: {.content-hidden}
{{< include ../macros.tex >}}

```{python}
import matplotlib as mpl, matplotlib.pyplot as plt, matplotlib.colors as mplcolors, matplotlib.patheffects as pe, matplotlib.ticker as ticker
import numpy as np
import sys
sys.path.append('../../notebooks')  # import neurodyn
from neurodyn import *
from matplotlib_tufte import *
setup()

mpl.use('svg')

def mktrans(fig, ax):
	"""Makes figure and axes transparent"""
	fig.patch.set_facecolor('#FFFFFF00')
	ax.patch.set_facecolor('#FFFFFF00')
```
:::

In this chapter, we introduce a toy model so that we can later study the mappings of embedding spaces. We stress that this toy model serves only for illustration purposes, and our results apply to any neural field. For this reason, we only cite or give short derivations of results used to understand the model, and the influence of the mappings that are later applied to it.

## Networks of neurons to neural fields {#sec-networks-of-neurons-to-neural-fields}

### Low-rank networks of neurons {#sec-low-rank-network}

The toy model we introduce is an instance of a low-rank neural network. Such models hold their name from the form of their connectivity matrix, which has a rank of (at most) $p$. We write the connectivity matrix as

$$
J_{ij} = \frac 1N \sum_{\mu=1}^p F_{\mu i} G_{\mu j}.
$$ {#eq-lowrankj}

The geometric view of these networks is that the recurrent currents lie in a $p$-dimensional subspace of $\R^N$ spanned by the vectors $\{\vec{F_1}, \cdots, \vec{F_p}\}$ (where $\vec{F_\mu}=(F_{\mu 1}, \cdots F_{\mu N})$), which therefore define a "natural" linear embedding (in the sense of @JazOst21) of the neural population activity.

Recalling the notation from the introduction, the $h_i(t)$ are neuron potentials (with initial condition $h_i(0)$), $\phi : \R \mapsto \R^+$ is the monotonic increasing activation function, and we write the evolution equation for this network of neurons:

$$
\dot h_i(t) = \underbrace{-h_i(t)}_\text{exponential decay} + \underbrace{\frac 1N \sum_{\mu=1}^p \sum_{j=1}^{N} F_{\mu i} G_{\mu j} \phi(h_j(t))}_\text{recurrent current $I^\text{rec}_i(t)$}.
$$ {#eq-lowrank-evolution}

The exponential decay term describes how, in isolation, a neuron's potential tends to zero, biologically corresponding to the decay of the membrane potential to its value at rest. The recurrent current term $I^\text{rec}_i(t)$ describes the "currents" received by neuron $i$ from all the other neurons in the network.

We would like to add a few remarks on the physical units of @eq-lowrank-evolution. Strictly speaking, a prefactor $\tau$ with units of $\mathrm{seconds}^{-1}$ should be multiplied with $\dot h_i(t)$ of units $\mathrm{volt}\cdot\mathrm{seconds}^{-1}$, such that the left-hand side of the equation is consistent with the unit of voltage. Similarly, the activation function has units $\mathrm{seconds}^{-1}$ and the connectivity matrix units of $\mathrm{coulomb}$, and a multiplicative term $R$ with units $\mathrm{ohm}$ should multiply the recurrent currents in order to yield a voltage. By rescaling time or connection weights, both $\tau$ and $R$ can be set to one, and so we omit to write them.

One last remark is that in our model, there is no external current; we only study the autonomous dynamics of the system.

### Gaussian $\mathcal{N}(\vec 0, \mathbb{1}_N)$ low-rank network of neurons

Let us now specify the low-rank model introduced in the previous section, and use the model introduced in @schmutz2023convergence, which can be seen as a simple example of a "Gaussian mixture low-rank network" @Beiran2021. This paper defines the low-rank vectors $F_{\mu i}$ such that each component independently samples a standard Gaussian (zero mean, unit variance). In other words, every neuron $i$ samples a vector $\vec{F_i} = (F_{1i}, \cdots, F_{pi})$ from the $p$-dimensional gaussian. We write

$$
\vec{F_i} = \vec{z_i},\ \vec{z_i} \stackrel{\text{iid}}{\sim} \rho(z_1, \cdots, z_p),
$$

where

$$
\begin{aligned}
\rho(z_1, \cdots, z_p) &= \prod_{\mu=1}^p \mathcal{N}(z_\mu) \\
\mathcal{N}(z) &= \frac{1}{\sqrt{2 \pi}} \mathrm{e}^{-\frac 12 z^2}.
\end{aligned}
$$

The vectors $G_{\mu i}$ are defined as

$$
G_{\mu i} = \tilde\phi(z_{\mu i}) \stackrel{\text{def}}{=} \frac{\phi(z_{\mu i}) - \avg{\phi(z_{\mu i})}}{\mathrm{Var}[\phi(z_{\mu i})]}.
$$

The motivation for this choice will be presented in a few paragraphs. Finally, the activation function remains to be defined. For simplicity, we take it to be the logistic function, although the following results are not sensitive to this choice,

$$
\phi(h) = \frac{1}{1 + \mathrm{e}^h}.
$$

Putting everything together, we get the following expression for the equation of evolution of the network of neurons:

$$
\dot h_i(t) = -h_i(t) + \frac 1N \sum_{\mu=1}^p \sum_{j=1}^{N} z_{\mu i} \tilde \phi(z_{\mu j}) \phi(h_j(t)) =: \mathrm{RHS}(h_1, \cdots, h_N).
$$ {#eq-toymodel-evolution}

We note that contrary to the model introduced in @schmutz2023convergence, we ignore the self-connections term, because it introduces a vanishing correction of order $\bO(1/N)$.

To understand this model better, we can analyze its fixed points and their stability. Here we summarize the results from the derivation presented in @sec-fixedpoints. We use the notation $h^\star$ to refer to fixed points of the network. The fixed points solve the roots of the evolution equation:

<!--
$$
\dot h_i(t)\ \text{evaluated at}\ h_i^\star = -h^\star_i(t) + \frac 1N \sum_{\mu=1}^p \sum_{j=1}^{N} z_{\mu i} \tilde \phi(z_{\mu j}) \phi(h^\star_j(t)) = 0
$$ 
-->

$$
\mathrm{RHS}(h^\star_1, \cdots, h^\star_N) = -h^\star_i + \frac 1N \sum_{\mu=1}^p \sum_{j=1}^{N} z_{\mu i} \tilde \phi(z_{\mu j}) \phi(h^\star_j) = 0
$$ 

* $h^\star_i = z_{\mu i}$ is a stable fixed point for all $\mu \in \{1,\cdots,p\}$. Additionally, because the Gaussian distribution is symmetric around zero, $h^\star_i = -z_{\mu i}$ are also stable fixed points. We refer to these fixed points as the "pattern" fixed points, because the network of neurons "remembers" the vectors $\vec{F_1}, \cdots, \vec{F_p}$.
* $h^\star_i=0$ is an unstable fixed point. It corresponds to all the neuron potentials being set to zero.

The analysis of stability from @sec-fixedpoints-stability can be summarized by the study of the eigenvalues of the matrix $K_{ij} = J_{ij} \partial \phi(h^\star_j) - \mathrm{Id}_{ij}$.

1. Taking Taylor expansions of the activation functions reveals that -- at least up to order 3 -- uneven powers tend to increase stability of the pattern fixed points (and reciprocally, decrease stability of the zero fixed point), and vice-versa for the even powers (with the notable exception of the constant offset, which doesn't play a role in this analysis).
2. A steeper slope at the origin of the activation function also seems to improve stability of the pattern fixed points. This corresponds to a sharper difference between the "inactive" (low potential) and the "active" (high potential) neurons.
3. The spectrum of $K$ is composed of $p$ eigenvalues (labeled $\lambda_\mu$ for all $\mu\in\{1,\cdots,p\}$) that depend on the fixed point, and $N-p$ eigenvalues $\lambda_\perp = -1$ corresponding to the orthogonal complement of the pattern fixed points.

In our case, this analysis in Taylor expansions was sufficient to explain the observed stability resulting from a logistic activation function.

We should however note that the results obtained in @sec-fixedpoints are valid in the $N \to \infty$ limit. A numerical analysis shows in @fig-spectrum that at $N=250$, the numerical eigenvalues approximate the analytical eigenvalues, and this correspondence improves when we take larger $N$ (typically we will for the rest of this thesis take $N > 10^3$).

```{python}
#| label: fig-spectrum
#| fig-cap: Spectrum of $K$ for $N=250$ and $p=2$. The numerical estimation (black, eigenvalues $\lambda_\mu$ and $\lambda_\perp=-1$) of the eigenvalues is close to the analytical ($N \to \infty$) derivation (red, only eigenvalues $\lambda_\mu$). Eigenvalues above the dotted gray line (positive values) correspond to unstable fixed points, and below (negative values) correspond to stable fixed points.
#| fig-subcap:
#|   - Pattern (stable) fixed point
#|   - Zero (unstable) fixed point
#| layout-ncol: 2
#| echo: false

dsigmoid = lambda x: sigmoid(x) * (1 - sigmoid(x))

def plot_spectrum(hstar_str: str):
	params = LowRankRNNParams.new_valentin(p=2, N=250, phi=sigmoid, I_ext=zero, exclude_self_connections=False)
	if hstar_str == 'pattern': hstar = params.F[:, 0]
	elif hstar_str == 'zero': hstar = np.zeros(params.N)
	dphi = dsigmoid(hstar)
	J = np.einsum('im,jm->ij', params.F, params.G * dphi[:, None]) / params.N - np.eye(params.N)
	eig_num = np.sort(np.linalg.eigvals(J).real)[::-1]
	eig_ana = { 'zero': 0.19061, 'pattern': -0.28090 }[hstar_str]  # see appendix A
	
	fig, ax = plt.subplots()
	mktrans(fig, ax)
	ax.axhline(0, color='gray', linestyle='--')
	ax.plot(eig_num[:8], marker='o', clip_on=False)
	ax.plot([7,8,8.5], eig_num[8:11], linestyle='--', color='k', clip_on=False)
	ax.plot(np.arange(params.p), np.full(params.p, eig_ana), color='r', marker='o', clip_on=False)
	ax.set_ylim((-1, 0.5))
	ax.set_xlim((0, 8.5))
	ax.set_ylabel('Eigenvalues')
	ax.set_xlabel('nth eigenvalue')
	breathe()
	return fig, ax

# %matplotlib inline

# pattern fixed points
fig, ax = plot_spectrum('pattern')
display(fig)

# zero fixed point
fig, ax = plot_spectrum('zero')
display(fig)
```

### Emergence of structure in the networks of neurons

In the introduction, we introduced the convergence of a network of neurons to a smooth neural field when the embedding space is well-chosen. For our toy model, it seems natural to try to embed the neurons according to their positions on the $p$-dimensional Gaussian.

#### Numerical aspects of simulating a low-rank network of neurons

We first demonstrate what happens with numerical simulations in the simple case of $p=1$. We generate a network of 50,000 neurons, where each neuron samples a one-dimensional Gaussian to get its position in the embedding. The function $\tilde \phi$ is computed by rescaling $\phi$ with the numerically computed mean and variance.[^code1]

Instead of numerically computing the full connectivity matrix $J$ (which would require storing $N^2 = 50,000^2 \sim 10^{10}$ entries, taking about 40 GiB if we use 32-bit floating point numbers... yikes !), we take advantage of its low-rank structure to compute recurrent currents, therefore only storing $p \times N$ numbers instead of $N^2$.[^code2]

[^code1]: In the source code, implementation for this can be found under `notebooks/neurodyn/_rnn.py`, in the method [`LowRankRNNParams.new_sampled_valentin`](https://github.com/ninivert/lcnthesis/blob/master/notebooks/neurodyn/_rnn.py#L154).

[^code2]: See the implementation in `notebooks/neurodyn/_rnn.py`, in the method [`LowRankRNN.I_rec`](https://github.com/ninivert/lcnthesis/blob/master/notebooks/neurodyn/_rnn.py#L302).

#### Naive and natural embeddings for $p=1$

Initially, we set all the potentials to zero, then simulate the differential equation by using the Runge-Kutta of order 4 integration schema (see [`scipy.integrate.solve_ivp`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.solve_ivp.html)). In @fig-p1, we show the resulting dynamics for a few thousand neurons. We order the neurons as they appear in the numerical array (a naive "random" one-dimensional embedding), and compare this to the ordering of neurons by their sampled position $z_1 \sim \mathcal{N}$.

::: {.content-visible when-format="html"}
![Simulation of the Gaussian network of neurons with $p=1$. Every neuron is represented by a dot, of which the height represents its activity level, and the colorbar doubles as a y-axis.](figures/embedding_p=1.mp4){#fig-p1}
:::

::: {.content-visible when-format="pdf"}
![(TODO) Simulation of the Gaussian network of neurons with $p=1$. Every neuron is represented by a dot, of which the height represents its activity level, and the colorbar doubles as a y-axis.](figures/lol7.jpg){#fig-p1}
:::

The results of the simulation show that the random ordering of the neurons does not yield a smooth embedding, in contrast to the smooth surface that appears when we order the neurons by their position $z_{1i}$. The simulation serves as a demonstration that the neurons converge to the stable fixed point $h^\star_i = z_{1i}$, as well as confirming that the zero fixed point is unstable. Indeed, despite the zero initialization, the simulation did not stay there because small numerical fluctuations were amplified by the instability. 

The observed smooth surface corresponds to the activation function $\phi(h^\star) = \phi(z_1)$. Therefore, the Gaussian patterns define a natural embedding of the network, and the neural field at the fixed point reads $h^\star(z_1, t) = z_1$. We can write the neural field equation for $p=1$ in the Gaussian embedding as 

$$
\partial_t h(z_1, t) = -h(z_1, t) + \int_{\R} z_1 \tilde\phi(y_1) \phi(h(y_1, t)) \mathcal{N}(\d y_1).
$$ {#eq-nf-gaussian-p1}

#### A natural embedding for $p=2$

Because @eq-nf-gaussian-p1 already involves a one-dimensional integral, the $[0,1]$ embedding can easily be found by applying rescaling methods, such as the inverse CDF method described in @sec-mapping-cdf later in this chapter.

The question of finding a one-dimensional embedding becomes non-trivial when we consider $p>1$, here we take $p=2$. In @fig-p2, we repeat the same simulations as in the previous section. Again, the structure of the network emerges naturally as a smooth surface in $\R^2$.

::: {.content-visible when-format="html"}
![Simulation of the Gaussian network of neurons with $p=2$. Every neuron is represented by a dot, of which the height represents its activity level. Projections are shown by gray points.](figures/embedding_p=2.mp4){width=500px #fig-p2}
:::

::: {.content-visible when-format="pdf"}
![(TODO PDF) Simulation of the Gaussian network of neurons with $p=2$. Every neuron is represented by a dot, of which the height represents its activity level. Projections are shown by gray points.](figures/lol7.jpg){width=500px #fig-p2}
:::

We however note that in this case, there are two pattern fixed points that the network can converge to. In the example given here, the network coincidentally converged again to the first pattern fixed point. The network is in the $\mu=1$ pattern, which we can see because the neural field is constant in the $z_2$ direction. We write the fixed point $h^\star(z_1, z_2, t) = z_1$. The neural field is now written as

$$
\begin{aligned}
\partial_t h(z_1, z_2, t) &= -h(z_1, z_2, t)\\
&+ \int_{\R} \left(z_1 \tilde\phi(y_1) + z_2 \tilde\phi(y_2)\right) \phi(h(y_1, y_2, t)) \mathcal{N}(\d y_1) \mathcal{N}(\d y_2).
\end{aligned}
$$ {#eq-nf-gaussian-p2}

More generally, we can write the $p$-dimensional neural field, where $\vec z \in \Rp$:

$$
\begin{aligned}
\partial_t h(\vec z, t) &= -h(\vec z, t) + \int_{\Rp} w(\vec z, \vec y) \phi(h(\vec y, t)) \mathcal{N}^p(\d \vec y) \\
w(\vec z, \vec y) &= \sum_{\mu=1}^p z_\mu \tilde\phi(y_\mu) \\
\mathcal{N}^p(\d \vec y) &= \frac{\mathrm{exp}(\frac12\sum_{\mu=1}^p z_\mu^2)}{(2\pi)^{p/2}} \mathrm{d} y_1 \cdots \mathrm d y_p.
\end{aligned}
$$ {#eq-nf-gaussian-p}

## Simulation schemes for neural fields {#sec-simnf}

Now that we have introduced the analytical neural fields, let us discuss how we can simulate them numerically, more specifically how we can estimate the $p$-dimensional integral.

### Sampling method (Monte Carlo integration) {#sec-mc}

Due to the formulation of the neural field weighed by a probability distribution $\rho(y_1, \cdots, y_p)$, a natural way to estimate the integral is by Monte Carlo integration.
The essence of this method is to use the Central Limit Theorem, where we take $N$ independent samples $\vec{y_i} = (y_{1i}, \cdots, y_{pi})$ from the distribution $\rho$, and use them to estimate the integrand.

$$
\begin{aligned}
\mathcal{I}(\vec z) &= \int_{\Rp} \underbrace{w(\vec z, \vec y) \phi(h(\vec y, t))}_{I(\vec z, \vec y)} \rho(\d \vec y) \\
\hat{\mathcal{I}}(\vec z) &= \frac 1N \sum_{i=1}^N I(\vec z, \vec{y_i}), \, \vec{y_i} \stackrel{\text{iid}}{\sim} \rho
\end{aligned}
$$ {#eq-mc}

Applying the Central Limit Theorem, we get that the estimation $\mathcal I(\vec z)$ from @eq-mc has a convergence rate of $\bO(1/\sqrt{N})$, since

$$
\begin{aligned}
\mathrm{Var}\left[\hat{\mathcal{I}}(\vec z) \right] &= \frac 1N \mathrm{Var}_{\vec y \sim \rho}[I(\vec z, \vec y)].
\end{aligned}
$$

<!-- The attentive reader might have noticed by now that the Monte-Carlo method just reduces to simulating the network of neurons @eq-toymodel-evolution. Effectively, the network of neurons converges to the neural field with rate $\bO(1/\sqrt{N})$.

One small nuance we should add, is that strictly speaking, Monte-Carlo integration should resample the integrand at every timestep, but the network of neurons samples the neurons once, and propagates these points through time. -->

### Grid method (Trapezoidal integration) {#sec-gridmethod}

Another method to estimate an integral numerically is by discretizing the integrand on a regular grid. We simply state the well-known result that the convergence rate of the estimation is $\bO(N^{-2/p})$. Interestingly, the convergence rate depends on the dimension $p$ of the integral. For $p \geq 4$, Monte-Carlo integration shows better scaling behaviour (but depending on the use case, prefactors of the error can change which method is better).

In our case, since the distribution $\rho(z_1, \cdots, z_p)$ can be factorized into the product of $p$ independent Gaussian distributions, we can decouple the $p$-dimensional integral into the product of $p$ one-dimensional integrals. Then the error estimation of the integral scales as $\bO(N^{-2})$. For this reason, we use the grid method for the remainder of this thesis.

## Characterizing dynamics of the low-rank neural field

We now introduce some tools that will help us understand the toy model. We stress that in general, other neural field models might not have this luxury, and that we only make use of these tools to help explain the dynamics of the simulated neural fields. The overlaps and projections introduced here are only computed from the results of the simulation (see [`overlap.py`](https://github.com/ninivert/lcnthesis/blob/master/notebooks/neurodyn/_overlap.py) in the source code).

### Overlap variables

Overlap variables in continuous time variables were introduced by @Gerstner_1992 to study spiking networks models of associative memory (Hopfield-type networks), which can be seen as a low-rank network with discrete spatial structure instead of our continuous $\vec{z}$. The overlap variables measure the correlation between the state of the network (the value of the potential $h(\vec z, t)$ at each point in space) and the pattern fixed point.

For the network for neurons, we write the overlap with the pattern $\mu$ as

$$
\begin{aligned}
m_\mu(t) &= \frac 1N \sum_{i=1}^N \tilde \phi(z_{\mu i}) \phi(h_i(t)) \\
&\xrightarrow{N \to \infty} \int_{\Rp} \tilde \phi(z_\mu) \phi(h(\vec z, t)) \mathcal{N}^p(\d \vec z)
\end{aligned}
$$ {#eq-overlap}

We note that similarly to @Gerstner_1992, we can write the dynamics of the neural field in terms of the overlap variables:

$$
\partial_t h(\vec z, t) = -h(\vec z, t) + \sum_{\mu=1}^p z_\mu m_\mu(t).
$$ {#eq-overlap-evolution}

This formulation "hides" the integral inside the overlap variables, but one should not forget that the overlaps depend on the neural field at any given time.

### Low-rank neural fields as $p$-dimensional closed systems {#sec-closed-system}

@eq-overlap-evolution hints towards the possibility of writing the dynamics of the low-rank system as a closed system of $p$ variables. As explained in @sec-low-rank-network, the recurrent currents span a $p$-dimensional subspace, and the orthogonal component behaves independently. This motivates the formulation of a $p$-dimensional closed system.

We refer to @veltz2009localglobal, section 4.3 for the reduction of general low-rank networks to $p$-dimensional closed systems, and give the equations applied to our toy model. Defining the projections $\kappa_\mu(t)$ of the neural field onto the patterns

$$
\kappa_\mu(t) = \int_{\Rp} y_\mu h(\vec y, t) \mathcal{N}^p(\d \vec y),
$$ {#eq-kappa}

we can decompose the neural field onto the basis of patterns (mathematically, the patterns form an orthonormal basis of functions):

$$
h(\vec z, t) = h^\perp(\vec z, t) + \sum_{\mu=1}^p \kappa_\mu(t) z_\mu
$$

The orthonormal component $h^\perp(\vec z, t)$ is independent of the rest of the system and decays exponentially. Then, the equations of evolution for the projections are given by the following closed system:

$$
\begin{aligned}
\dot \kappa_\mu(t) &= -\kappa_\mu(t) + \int_{\Rp} \tilde\phi(y_\mu) \phi(h(\vec y, t)) \mathcal{N}^p(\mathrm d \vec y) = -\kappa_\mu(t) + m_\mu(t) \\
\kappa_\mu(0) &= \int_{\mathbb{R}^p} y_\mu h(\vec y, 0) \mathcal{N}^p(\mathrm d \vec y),
\end{aligned}
$$

and the orthogonal component evolves according to

$$
\begin{aligned}
h^\perp(\vec z, t) &= h^\perp(\vec z, 0) \mathrm e^{-t}\\
h^\perp(\vec z, 0) &= h(\vec z, 0) - \sum_{\mu=1}^p \kappa_\mu(0) z_\mu.
\end{aligned}
$$

The set of $\kappa_\mu(t)$ defines a trajectory in a $p$-dimensional *latent space*. We note that similarly to @eq-overlap-evolution, the overlaps intervene in the equations of evolution, and carry the information on the neural field.

Additionally, it can be seen from the expression of the overlaps in @eq-overlap, that in the case of a linear activation function $\phi(h) = c_0 + c_1 h$, we have the equality $m_\mu(t) = \kappa_\mu(t)$.

## A cycling neural field {#sec-cycling-nf}

In @sec-networks-of-neurons-to-neural-fields we introduced a simple toy model of a neural field with a natural embedding on the $p$-dimensional Gaussian. Analytical derivations accompanied by numerical simulations showed the dynamics of this model can be summarized by the convergence to pattern fixed points. In this section, we modify the toy model minimally, such that we can observe more interesting behaviour in the form of cycling.

We modify @eq-nf-gaussian-p in two ways:

1. We define a time delay $\delta$ with which a neuron will "wait" before reacting to a change in its potential.
2. We define "rolling" as the response of a neuron to shift towards the "next" pattern given its state matching a current pattern. This is done by adding a shift $\mu+1$ in the connectivity kernel, with the convention $p+1=1$ (see @Ami89, Chapter 5).

The resulting cycling neural field is written as such:

$$
\partial_t h(\vec z, t) = -h(\vec z, t) + \sum_{\mu=1}^p \int_{\mathbb{R}^p} z_{\mu+1} \tilde \phi (y_\mu) \phi(h(\vec y, t - \delta)) \mathcal{N}^p(\mathrm d \vec y)
$$ {#eq-nf-cycling-p}

The geometric intuition behind this formulation is that now the recurrent currents are "rotated by half a turn" (around the axis normal to the plane $(z_\mu, z_{\mu+1})$) in the embedding. When the network is at a pattern fixed point $\mu$, the recurrent drive will then push it towards the pattern $\mu+1$. @fig-cycling-irec shows the difference in the behaviour of the original and the cycling neural field.

```{python}
#| label: fig-cycling-irec
#| fig-cap: Intuition for the cycling behavior. We consider the network in the state $h(z_1,z_2)=z_1$, and look at the recurrent currents. The recurrent currents align with the activity for the original neural field, and the effective change in potential is zero everywhere. For the cycling neural field, the recurrent currents are orthogonal to the activity, such that with the decay term, the effective change leads to a rotation in the counter-clockwise direction, towards the state $h(z_1,z_2)=z_2$.
#| echo: false

from matplotlib.image import NonUniformImage

nrec = 6
rnn_nocyc = LowRankRNN.new_sampled_valentin(
	phi=sigmoid, mapping_samples=ReshapeMapping.new_nrec(nrec=nrec).inverse_samples(),
	I_ext=zero, exclude_self_connections=False
)
rnn_cyc = LowRankCyclingRNN.new_sampled_valentin(
	phi=sigmoid, mapping_samples=ReshapeMapping.new_nrec(nrec=nrec).inverse_samples(),
	I_ext=zero, exclude_self_connections=False, delta=10, shift=1
)
rnn_cyc.h_lagging = lambda t, h: h  # HACK : make the recurrent currents work for t=0

fig, axes = plt.subplots(ncols=3, nrows=2, figsize=(3*4-3, 2*3), layout='constrained')

for i, rnn in enumerate([rnn_nocyc, rnn_cyc]):
	axes[i, 0].set_aspect('equal')
	axes[i, 0].set_xlabel('$z_1$')
	axes[i, 0].set_ylabel('$z_2$')
	shape = (2**nrec, 2**nrec)
	im = axes[i, 0].pcolormesh(
		rnn.F[:, 0].reshape(shape), rnn.F[:, 1].reshape(shape), rnn.phi(rnn.F[:, 0]).reshape(shape),
		cmap=cmap_bi, vmin=0, vmax=1,
		rasterized=True  # enable rasterization to prevent 50Mo SVG files, lol
	)
	breathe(axes[i, 0])
	if i == 1:
		plt.colorbar(im, ax=axes[i, 0], orientation='horizontal', label='$\\phi(h(z_1,z_2))$')
	
	axes[i, 1].set_aspect('equal')
	axes[i, 1].set_xlabel('$z_1$')
	axes[i, 1].set_ylabel('$z_2$')
	im = axes[i, 1].pcolormesh(
		rnn.F[:, 0].reshape(shape), rnn.F[:, 1].reshape(shape), rnn.I_rec(0, rnn.F[:, 0]).reshape(shape),
		cmap=cmap_pan, vmin=-2, vmax=2,
		rasterized=True  # enable rasterization to prevent 50Mo SVG files, lol
	)
	breathe(axes[i, 1])
	if i == 1:
		plt.colorbar(im, ax=axes[i, 1], orientation='horizontal', label='$I^\mathrm{rec}(z_1, z_2)$')

	axes[i, 2].set_aspect('equal')
	axes[i, 2].set_xlabel('$z_1$')
	axes[i, 2].set_ylabel('$z_2$')
	im = axes[i, 2].pcolormesh(
		rnn.F[:, 0].reshape(shape), rnn.F[:, 1].reshape(shape), rnn.dh(0, rnn.F[:, 0]).reshape(shape),
		cmap='coolwarm', vmin=-2, vmax=2, shading='gouraud',
		rasterized=True  # enable rasterization to prevent 50Mo SVG files, lol
	)
	breathe(axes[i, 2])
	if i == 1:
		plt.colorbar(im, ax=axes[i, 2], orientation='horizontal', label='$\partial_t h(z_1, z_2)$')

add_headers(axes=axes.flat, row_headers=['Original neural field', 'Cycling neural field'], fontsize='medium', row_pad=8)
add_headers(axes=axes.flat, col_headers=['Neural activity', 'Recurrent currents', 'Effective change'], fontsize='medium', col_pad=8)

for ax in axes.flat:
	mktrans(fig, ax)

# %matplotlib inline
display(fig)
```

We note however that @eq-nf-cycling-p no longer is a partial differential equation, but rather a delayed differential equation, which are in general much more complicated to solve. A simplifying assumption that we make is we extend the initial condition back in time, such that $h(\vec z, t < 0) = h(\vec z, 0)$ is a constant function. Numerically, this also means that we have to store a history of the neural fields instead of only the current one. Since the [`solve_ivp`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.solve_ivp.html) method dynamically adapts the timestep of integration, we use linear interpolation between two known points in the stored neural field states to estimate $h(\vec z, t-\delta)$ (see [`lagging.py`](https://github.com/ninivert/lcnthesis/blob/master/notebooks/neurodyn/_lagging.py) and [its use](https://github.com/ninivert/lcnthesis/blob/master/notebooks/neurodyn/_rnn.py#L334)).

With these considerations, we simulate a $p=2$ cycling neural field with $\delta=6$ (since the membrane time constant has been fixed to one, this means that $\delta = 6\tau = 6$) and initial condition $h(z_1, z_2, t < 0) = z_1$ in @fig-cycling-anim. The latent trajectory is estimated from the simulation results, and animated simultaneously with the neural field. We clearly see the oscillations between the patterns $\mu=1$ and $\mu=2$.

::: {.content-visible when-format="html"}
![A cycling neural field with $\delta=6$ and initial condition $h(z_1, z_2, t < 0) = z_1$. The latent trajectory is also computed, and demonstrates the oscillatory behaviour.](figures/embedding_p=2_cycling.mp4){#fig-cycling-anim}
:::

::: {.content-visible when-format="pdf"}
![(TODO PDF) A cycling neural field with $\delta=6$ and initial condition $h(z_1, z_2, t < 0) = z_1$. The latent trajectory is also computed, and demonstrates the oscillatory behaviour.](figures/lol7.jpg){#fig-cycling-anim}
:::

Relating to @sec-closed-system, the dynamics of the projection now read:

$$
\dot \kappa_\mu(t) = -\kappa_\mu(t) + m_{\mu+1}(t - \delta).
$$

<!-- This system of equations can be solved by steps of $\delta$, iteratively plugging in the previous solution to solve each cycle. -->

<!-- When the neural field starts in the pattern $\nu$, that is $h(\vec z, 0) = z_\nu$, this translates to $\kappa_\nu(t \leq \delta) = 1$, and all other $\kappa_\mu(t \leq \delta) = 0,\,\mu\neq\nu$. Additionally, let us suppose that $\phi$ is linear, so that $m_\mu(t) = \kappa_\mu(t)$, and let us take $p=2$. Then we can solve the equation for the "first cycle" $0 < t \leq \delta$:

$$
\begin{aligned}
\kappa_1(t) &= \mathrm{e}^{-t} \\
\kappa_2(t) &= 1 - \mathrm{e}^{-t}.
\end{aligned}
$$

With this, we see that the projections "alternate" back and forth with period $\delta$. -->

## Mapping $\R^2$ to $[0,1]^2$ using the CDF {#sec-mapping-cdf}

In preparation to the next chapter, we first map the 2-dimensional neural field equation in $\R^2$ to $[0,1]^2$, and, in general $\Rp$ to $[0,1]^p$. There are many ways to do this by using functions $\R \mapsto [0,1]$, but a practical choice is the Gaussian CDF (Cumulative Density Function), because it has the benefit of absorbing the density into the kernel, in the sense that the integral $[0,1]^p$ becomes weighted by the uniform distribution. The one-dimensional Gaussian CDF is defined as

$$
\CDF(z) = \int_{-\infty}^z \mathcal{N}(\d y)
$$

and maps $\R$ to $[0,1]$,[^R-to-01] as pictured in @fig-gaussian-cdf.

```{python}
#| label: fig-gaussian-cdf
#| fig-cap: CDF of the Gaussian distribution
#| echo: false
#| 
from scipy import stats

z = np.linspace(-4.5, 4.5, 200)
fig, ax = plt.subplots()
ax.plot(z, stats.norm.cdf(z), clip_on=False)
# ax.set_xlim((-4, 4))
ax.set_ylim((0, 1))
ax.set(xlabel='$z$', ylabel='$\\mathrm{CDF}(z)$')
ax.set_yticks([0.0, 0.25, 0.5, 0.75, 1.0])
ax.grid(clip_on=False)
# ax.grid(clip_on=False, axis='y')
# breathe(ax, which='x')
# breathe(ax, which='y', pad_frac=0.1)
breathe(ax)
mktrans(fig, ax)
display(fig)
```

[^R-to-01]: Strictly speaking the image of $\R$ is in $(0,1)$, but for simplicity we consider $\R \cup \{-\infty, +\infty\}$ which maps to $[0,1]$. This is not fundamentally important, because numerically the infinities are never touched, and analytically the recurrent currents given by the integration vanish at infinity due to the Gaussian weighting.

Defining the change of variables $v_\mu=\CDF(z_\mu), u_\mu=\CDF(y_\mu)$, we can define the neural field $h_U$ and the connectivity kernel $w_U$ for the uniform distribution as

$$
\begin{aligned}
h_U(\vec v, t) &= h(\CDF^{-1}(\vec v), t) = h(\CDF^{-1}(v_1), \cdots, \CDF^{-1}(v_p)) \\
w_U(\vec v, \vec u) &= w(\CDF^{-1}(\vec v), \CDF^{-1}(\vec u)).
\end{aligned}
$$ {#eq-hu-wu}

After the change of variables, the neural field in $[0,1]^p$ is equivalent to the neural field in $\Rp$, and its equation reads

$$
\partial_t h_U(t, \vec v) = -h_U(t, \vec v) + \int_{[0,1]^p} w_U(\vec v, \vec u) \phi(h_U(t, \vec u)) \mathrm d \vec u.
$$

Defining the neural field in $[0,1]^p$ also addresses the problem that we tried to hide in @sec-gridmethod relating to the difficulties of defining a grid on $\Rp$. With the integral on $[0,1]^p$, we can simply use a uniform grid, which maps back to samples in $\Rp$ by using the inverse CDF, as shown in @fig-grid-correspondance.

```{python}
#| label: fig-grid-correspondance
#| fig-cap: A grid in $[0,1]^2$ and the corresponding samples in $\R^2$. The numeric density of points in $\R^2$ approaches the normal distribution as the grid becomes finer (here, the mesh size is $2^{-4}$ along each dimension).
#| echo: false

nrec = 4
mapping = ColumnMapping.new_nrec(nrec=nrec)
Y = mapping.inverse_samples()
Z = stats.norm.ppf(Y)

fig, axes = plt.subplots(ncols=2, figsize=(8, 3))
axes[0].scatter(*Y.T, c='k', s=15)
axes[1].scatter(*Z.T, c='k', s=15)
axes[0].set_xlim((0,1))
axes[0].set_ylim((0,1))
axes[0].set_aspect('equal')
axes[1].set_aspect('equal')
axes[0].set_title('In $[0,1]^2$ (CDF space)', fontsize='medium')
axes[1].set_title('In $\\mathbb{R}^2$ (PDF space)', fontsize='medium')
breathe(axes[0])
breathe(axes[1])
mktrans(fig, axes[0])
mktrans(fig, axes[1])
display(fig)
```

We make the final remark that this method of mapping $\Rp$ to $[0,1]^p$ by using the CDF does not work for any neural field. In our case, we use the fact that the distribution factorizes into $\rho(z_1, \cdots, z_p) = \rho_1(z_1) \cdots \rho_p(z_p)$, such that the components of $\vec{v}$ are independent. We can then define the "componentwise CDF" that relates to the total CDF via

$$
\begin{aligned}
\CDF(z_1, \cdots, z_p) &= \int_{-\infty}^{z_1} \cdots \int_{-\infty}^{z_p} \rho(\d y_1, \cdots, \d y_p) \\
&= \int_{-\infty}^{z_1} \rho_1(\d y_1) \cdots \int_{-\infty}^{z_p} \rho_p(\d y_p) \\
&= \CDF(z_1) \cdots \CDF(z_p) \\
&= v_1 \cdots v_p = \prod_{\mu=1}^p v_\mu.
\end{aligned}
$$