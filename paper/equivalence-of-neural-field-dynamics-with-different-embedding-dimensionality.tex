%% NETN Template file

%% Documentclass:
\documentclass[NETN,manuscript]{stjour-new}

%% Or,
%% For production only, need .otf fonts, private MIT logo files,
%% and need to use lualatex:
% \documentclass[finalfonts,NETN]{stjour-new}

%%%%%%%%%%% Article Set-Up %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Article Type:

%% Choose one of these options, (Default is Research).
%% Research
%% Methods 
%% Data
%% Review
%% Perspectives

\articletype{Research}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% author definitions should be placed here:

\def\R{\mathbb R}
\def\RR{\R^2}
\def\Rp{\R^p}
\def\d{\mathrm d}
\def\Jab{\tilde J_{\alpha\beta}}
\def\Fab{\tilde F_{\alpha\beta}}
\def\Gab{\tilde G_{\alpha\beta}}
\def\Jij{J_{ij}}
\def\Fij{F_{ij}}
\def\gij{G_{ij}}
\def\wu{w_\textrm{U}}
\def\hu{h_\textrm{U}}
\def\bO{\mathcal{O}}
\def\CDF{\mathrm{CDF}}
\newcommand{\avg}[1]{\langle{#1}\rangle}
\newcommand{\norm}[1]{\lVert{#1}\rVert}
% \def\avg#1{\langle{#1}\rangle}
\renewcommand{\vec}[1]{\boldsymbol{#1}}


\begin{document}

% TODO: what title is better ?
% \title{Equivalence of neural field dynamics with different embedding dimensionality}
\title{Neural fields with different embedding dimensionality can express identical dynamics}
% \title{Neural dynamics can be equivalently embedded in neural fields of different dimensions}

% \subtitle{<Subtitle Here>} %% Optional subtitle

%% If shortened title for running head is needed so that the article title can fit
%%   in the running head, use [] argument, ie,
%%
%%   \title[Shortened Title for Running Head]{Title of Article}
%%   \subtitle{Subtitle Here}

%%% Author/Affil
%% Since we use \affil{} in the argument of the \author command, we
%% need to supply another version of the author names, without \affil{}
%% to be used for running heads:

\author{Nicole Vadot\affil{1}, Valentin Schmutz\affil{1}, \and Wulfram Gerstner \affil{1}}

\affiliation{1}{LCN, EPFL, Lausanne, Switzerland}

%ie.
%\affiliation{1}{Gatsby Computational Neuroscience Unit, University
%College London, London, United Kingdom} 

% \affiliation{2}{Another Department, Institution, City, Country}

%ie
%\affiliation{2}{Center for Studies in
%Behavioral Neurobiology, Concordia University, Montreal, Quebec,
%Canada}

\correspondingauthor{Nicole Vadot}{nicole.vadot@epfl.ch}
\correspondingauthor{Valentin Schmutz}{valentin.schmutz@epfl.ch}
\correspondingauthor{Wulfram Gerstner}{wulfram.gerstner@epfl.ch}

% ie,
%\correspondingauthor{Ritwik K. Niyogi}{ritwik.niyogi@gatsby.ucl.ac.uk}

\keywords{Neural fields, Embedding, Dimensionality}

%ie
%\keywords{Work, Leisure, Normative, Microscopic,  Reinforcement Learning, Economics}

\begin{abstract}
Classical neural field models describe interactions between neurons positioned in an embedding; often a physical space (for instance, that of the cortical sheet) or, later, abstract spaces (for instance, that of neuron celltypes), in which the dynamics are highly regular in space and time. Recent improvements in recordings of neural activity have further demonstrated the smoothness of the neural dynamics, when placed in an appropriate embedding space.

Decoupling the embedding space from the physical position of neurons begs the question of uniqueness of the embedding space. Given an embedding space and neural dynamics theron, are there other embedding spaces which can express the same behavior?

We show that embedding spaces are not unique, and more suprisingly, that we can find multiple embedding spaces in different dimensions, which all model identical neural dynamics.
The core of this work lies in the study of bijective mappings $S$ between $[0,1]^2$ and $[0,1]$, which allows mapping a two-dimensional neural field to an equivalent one-dimensional neural field.
This procedure is easily generalized to map between embeddings in arbitrary number of dimensions.
We introduce a measure of locality, which quantifies how the regularity of the neural fields in conserved through the mappings $S$. This allows us to show that the resulting neural field is well-behaved, and can therefore be approximated using classical grid-based methods.
To illustrate these results through numerical models, we describe a coarse-graining procedure enforcing the locality of the numerically applied mappings.
\end{abstract}

% \begin{authorsummary}
% Author Summary here.
% \end{authorsummary}

\section{Introduction}

% NOTE: the goal here is to give an intuitive understanding of the problem,
% kind of in the same "story-telling"-way that I did in the master thesis.
% This also allows to introduce some of the notation that I use later on.

% \subsection{Neural field models in neuroscience}

Early recordings of cats' somatosensory \citep{Mou57} and visual \citep{HubWie62} cortices suggested that the cortical sheet is organized in vertical columns of functionally similar neurons.

The columnar organization of the cortex motivated the design of spatially structured models of neural population dynamics, namely neural field models \citep{WilCow73,Nun74,Ama77}, where the spatial dimensions corresponded to the two dimensions of the cortical sheet. 

As models of spatiotemporal neural population dynamics on the cortical sheet, neural field models can be used to explain experimentally observed patterns of cortical activity, such as travelling waves in visual \citep{SatNau12,MulRey14}, somatosensory \citep{PetHah03,FerBol06}, motor \citep{RubRob06,TakKim15}, and hippocampal \citep{LubSia09,PatFuj12,PatSch13} cortices (see \citep{MulCha18} for a review), and are used to model large-scale brain signals, such as electroencephalography (EEG) recordings \citep{Bre17} (see also \citep{DipRan18} for an example of neural field modelling of calcium imaging recording of the visual cortex).

More recent recording methods, especially in rodents, have provided evidence of low-dimensional organization of neuronal activity that does not depend on the physical locations of neurons on the cortical sheet. For example, contrary to cats' visual cortex, the orientation selectivity of pyramidal neurons in a rat's visual cortex does not depend on their locations, that is, orientation selectivity is heterogeneous at any given location \citep{OhkChu05}. Moreover, the activity of interneurons in any small volume of the visual cortex is also heterogeneous and seems to be structured by a low-dimensional manifold of fine cell subtypes \citep{BugDuf22}. The local functional heterogeneity of neuronal activity in the cortex challenges the old concept of functionally homogeneous cortical columns and, with it, classical neural field models. 

While, historically, the spatial dimensions in neural field models have been the two dimensions of the cortical sheet (with the notable exception of the ring models for orientation selectivity in the visual cortex \citep{BenBar95} and the head-direction system \citep{Zha96}), the "space" in neural field models does not need to represent physical space but can represent any suitable abstract embedding space. Neural field models with abstract embedding spaces could constitute a generalization of classical neural field theory for cortical networks that are not solely structured by distances of neurons on the cortical sheet.

Moreover, this work is motivated by recent mathematical developments \citep{JabPoy21} in mean-field theories involving the theory of graphons. Informally, these results suggest that, as long as the connectivity scales as $1/N$ (where $N$ is the number of neurons), the dynamics of large networks can be described in the $N \to \infty$ limit by a neural field equation where the embedding is simply the interval $[0,1]$. The main contribution of this paper is the derivation of a method, allowing to explicitly write such a one-dimensional neural field, starting from a known higher-dimensional neural field.

% \subsection{Formulating a one-dimensional neural field}

% If we consider neural fields in abstract embedding spaces, which embedding space should we choose? More fundamentally, given some spatiotemporal dynamics, is there a "natural" choice for the embedding space?

% We show that the answer to the second question is highly nontrivial because even the dimensionality of the embedding space, for some given spatiotemporal dynamics, is not clearly defined. This means that it is possible to find two embeddings (each associated with a connectivity kernel) of different dimensions that give rise to identical dynamics.

% TODO: do we talk about the tau=1, R=1 ?
% TODO: do we talk about external currents ?

We begin our work by considering a neural field associated with a $p$-dimensional embedding $\Rp$. $h(\vec z, t)$ describes the potential of the neural population at position $\vec z=(z_1,\cdots,z_p)$ in the embedding. The connectivity kernel $w(\vec y, \vec z)$ describes the amplitude of the recurrent currents from the population at $\vec z$ to the population at $\vec y$. Importantly, $w(\vec y, \vec z)$ is a ``smooth'' function in both arguments. This is a natural consequence of the derivation of neural field equations (rigorous proofs of convergence of networks to neural fields are still an active area of study, and we may cite \citep{CheDua19,AGATHENERINE202286} for recent developments).
$\rho(\vec z)$ measures the number of neurons per unit volume of the embedding space, effectively weighing the recurrent currents. The monotonic increasing activation function $\phi(h)$ describes the firing rate of a given neuron (population) with potential $h$. The neural field then evolves according to \autoref{eq:nf-in-rp}:  

\begin{equation} \label{eq:nf-in-rp}
\partial_t h(\vec z, t) = \underbrace{-h(\vec z, t)}_\text{exponential decay} + \underbrace{\int_{\mathbb{R}^p} w(\vec z, \vec y) \phi(h(\vec y, t)) \rho(\vec y) \mathrm d \vec y}_\text{recurrent current}.
\end{equation}

Without loss of generality, a simple bijective mapping can be applied to reduce any $p$-dimensional neural field to a neural field on $[0,1]^p$, additionally redefining the connectivity kernel to absorb the density and determinant of the transformation, we arrive at \autoref{eq:nf-in-01p}, a general form for any neural field equation in $p$ dimensions:

% The embedding space $\Rp$ (or any $p$-dimensional space) can be mapped to $[0,1]^p$ via bijective functions between the two spaces. For instance, if the neuron density factorizes as $\rho(z_1, \cdots, z_p)= \rho_1(z_1) \cdots \rho_p(z_p)$, then the associated cumulative density functions can be used to perform a change of variables, resulting in the an equivalent "uniform" neural field equation:

\begin{equation} \label{eq:nf-in-01p}
\partial_t h_U(t, \vec v) = -h_U(t, \vec v) + \int_{[0,1]^p} w_U(\vec v, \vec u) \phi(h_U(t, \vec u)) \mathrm d \vec u.
\end{equation}

This form highlights the central role of the connectivity kernel $w_U$, which together with the initial condition $h_U(t=0, \vec v)$, uniquely determines the neural dynamics.

% If the neuron density does not factorize, then one can arrive at the same formulation by redefining the connectivity kernel to include the neuron density and the determinant of the transformation. [TODO: this sentence might need some work]

In particular for $p=2$, we show that for any two-dimensional neural field equation (that is, with two spatial dimensions), there is a one-dimensional neural field equation, where the embedding space is simply the interval $[0,1]$, from which the solution to the two-dimensional equation can be fully reconstructed. The mapping from the two-dimensional equation to the one-dimensional equation is done via well-known measurable bijections between the square $[0,1]^2$ and the interval $[0,1]$. Importantly, these bijections are not diffeomorphisms.
% Also, we show that although the connectivity kernel of the one-dimensional equation seems to have a fractal structure, it is sufficiently regular for its solution to be numerically approximated using standard grid-based simulations.
The main idea is to define a measurable bijective mapping $S : [0,1]^2 \mapsto [0,1]$ and a measure $\lambda : [0,1]^2 \mapsto \mathbb{R}^+$, such that we can write a one-dimensional neural field	$\tilde h(\alpha, t)$ with a connectivity kernel defined as $\tilde w(\alpha, \beta) = w_U(S^{-1}(\alpha), S^{-1}(\beta))$. Doing this, we arrive at the following one-dimensional neural field equation: 
% The question of well-posedness of the neural field in \autoref{eq:nf-in-01} is defered in \autoref{sec:proof-equivalence}.

\begin{equation} \label{eq:nf-in-01}
\partial_t \tilde h(\alpha, t) = -\tilde h(\alpha, t) + \int_{[0,1]} \tilde w(\alpha, \beta) \phi(\tilde h(\beta, t)) \left[\lambda \circ S^{-1}\right](\d \beta).
\end{equation}

The difficulty in writing the one-dimensional neural field, is that the spatial structure of the original neural field in $[0,1]^2$ may be lost. The intuition is that we would like populations close in $[0,1]^2$ to be bijectively mapped to populations close in $[0,1]$. However, since $S^{-1}$ is a bijection, it cannot, by Netto's theorem, also be continuous. The core of our argument lies in the fact that we can relax the requirement that the spatial embedding is continuous, instead, we only require it is ``continuous on average''. We quantify this by introducing ``locality'' $V(S^{-1})$, which measures if small neighbourhoods in $[0,1]^2$ are mapped to small neighbourhoods in $[0,1]$ by the mapping $S^{-1}$. Using locality, we then prove that the mean-field approximation is still valid, implying that we can simulate the one-dimensional neural field using classical grid-based methods.

% question of uniqueness, discretization error, mean field limit and locality, link with numerical simualtions. chap 1.4, chap 3.1

% "discretization error"

% chap 1.1, chap 2.1, 2.5

Our results suggest that the notion of spatial dimensionality in neural field dynamics is not well-defined if not associated with a constraint on the regularity of the neural fields (and the connectivity kernel). Thereby, by studying a simple toy model, this work illustrates the importance of the analytic notion of regularity when neural population dynamics over abstract continuous spaces are considered. 

\section{Mappings $S$ from $[0,1]^2$ to $[0,1]$}

\subsection{Motivation}

We motivate the upcoming study of the mappings $S$ by considering a grid discretization of the original two-dimensional space $[0,1]^2$. For this, we subdivide each axis into $2^n$ equal segments, where $n \in \mathbb{N}$ describes the grid resolution. Each segment has a length of $2^{-n}$, resulting in a grid constituted of $2^n \times 2^n = 4^n$ squares in $[0,1]^2$. To each square bin indexed $i \in \{1,\cdots,4^n\}$, we associate a position $\vec{v_i} = (v_{i,1}, v_{i,2})$ located within the bounds of the bin. Since everytime $n$ is incremented by one, every axis is subdivided into two smaller equal parts, the positions $v_{i,1}$ and $v_{i,2}$ naturally lend themselves to a binary notation. We also introduce $v_\mu^{(n)}$ to denote the $n$-bit truncation of the position $v_\mu \in [0,1]$, where $\mu = 1,2$. Additionally, we write the error of the trunctation as a small corrective factor $0 \leq c^{(n)} < 2^{-n}$. Using this notation and dropping the index $i$ in favor of readability, we write the components of $\vec{v} \in [0,1]^2$ as:

\begin{equation} \begin{aligned} \label{eq:positions-v}
v_1 &= v_1^{(n)} + c_1^{(n)} = \sum_{l=1}^n b_1^l 2^{-l} + c_1^{(n)} = 0.b_1^1 b_1^2 \cdots b_1^n + c_1^{(n)}, \\
v_2 &= v_2^{(n)} + c_2^{(n)} = \sum_{l=1}^n b_2^l 2^{-l} + c_2^{(n)} = 0.b_2^1 b_2^2 \cdots b_2^n + c_2^{(n)}.
\end{aligned}\end{equation}

It can easily be seen that $v_\mu^{(n)} \to v_\mu$ as $n\to\infty$. In general, the bits of the $v_\mu$ can be expressed as:

\begin{equation*}
b_\mu^l = \mathrm{Ind} \left\{ 2^{l-1}v_\mu - \left\lfloor 2^{l-1}v_\mu \right\rfloor \right\}
\end{equation*}

[TODO: add a figure here of the discretization here ? do we need to clarify explicitly the bounds of each square bin ?]

Sampling the original connectivity kernel at the points $\vec{v_i}$ and $\vec{v_j}$ yields a numerical connectivity matrix $J_{ij} := w_U(\vec{v_i}, \vec{v_j})$, which can be used to simulate the resulting population dynamics equation:

\begin{equation} \label{eq:population-dynamics}
\frac{\d}{\d t} h_i(t) = -h_i(t) + \sum_{j=1}^{4^n} J_{ij} \phi(h_j(t)), \quad \text{for all } i \in \{1,\cdots,4^n\}.
\end{equation}

\autoref{eq:population-dynamics} can be seen as the discretized version of \autoref{eq:nf-in-01p}, where we associate $h_i(t) := h_U(\vec{v_i}, t)$. Indeed, by construction of the original neural field, the discrete dynamics converge to the true solution of the neural field, because the error of the mean-field approximation in $[0,1]^2$ vanishes as we take finer grids, letting $n \to \infty$.

We would now like to derive a bijective mapping $S$, such that the same argument applies on the neural field in $[0,1]$. Intuively, we want $S$ to conserve the regularity of the neural field on $[0,1]^2$, since if the mean-field argument also carries over to the one-dimensional neural field, it seems likely that it will express identical dynamics.

\subsection{$S$ as the pointwise limit of a sequence of bijections $S^n$}

Let us define a sequence of bijective mappings $S^n$, acting on the $n$-bit truncations of $\vec{v_i}$, as well as the sequence $\alpha_i^{(n)} \in [0,1]$ of positions resulting from the image of $\vec{v_i^{(n)}}$. In the following, we again drop the index $i$ for clarity.

\begin{equation} \begin{aligned}
S^n &: (v_1^{(n)}, v_2^{(n)}) = (0.b_1^1 b_1^2 \cdots b_1^n, 0.b_2^1 b_2^2 \cdots b_2^n) \\
&\mapsto \alpha^{(n)} = 0.b^1 b^2 \cdots b^{2n}
\end{aligned} \end{equation}

We note that we write $\alpha^{(n)}$ using $2n$ bits, which is intuitively the lowest number of bits required so that $S^n$ remains a bijection. More than $2n$ bits could be allowed while maintaining bijectivity, but we omit this possibility, since they wouldn't strictly be required. Effectively, $S^n$ maps between $2\times n$ input bits and $2n$ output bits. In this formulation, it becomes clear that in order to be a bijection, every bit of the input must be used once and only once to form the output. Setting aside bit inversions, this means that $S^n$ defines a permutation of the $2n$ input bits. [TODO: are more details necessary ? I think this is clear enough, though in the thesis manuscript I got over this point in more detail]

If it exists, we then we define $\alpha = S(v_1, v_2)$ as the pointwise limit of the sequence $\{S^n(v_1, v_2)\}_{n \in \mathbb N}$ (Note: We may define an extension of $S^n$ acting on an arbitrary $(v_1, v_2) \in [0,1]^2$ as $S^n(v_1, v_2) := S^n(v_1^{(n)}, v_2^{(n)})$. This extension is obviously not bijective, but in the $n \to \infty$ limit the finite-$n$ truncation converges, such that the pointwise limit may be a bijection). In terms of the current formulation, pointwise convergence requires that the least signficiant bits (LSB) of the input be mapped to the LSB of the output, and identically the most significant bits (MSB) of the input be mapped to the MSB of the output [TODO: do we need a proof of this ?]. This ensures that small corrections in the input $\vec{v}$ results in small corrections in the output $\alpha$.

We will in the rest of this paper show that this simple formulation in terms of permutation of input bits is flexible enough to express mappings that have good properties such as ``locality'', while being simple enough to help build intuitions.



---

let us think of what happens when we apply the mapping on a discrete grid of squares in [0,1]², and where the squares end up on the segment.
the squares are bijectively mapped to segments on the unit line, i.e. it defines a permutation of the squares onto the segments, or as defining an "ordering" of the square bins in $[0,1]^2$. This is done using a mapping S : [0, 1]2 7 → [0, 1],
where we cover the [0, 1]2 space by following the order assigned to the populations in
the image [0, 1].
considering finer and finer grids, we look/see/observe where the squares end up, and may define a sequence of mappings $S^n$ acting on finer and finer grids.
if the sequence converges pointwise, we then define $S$ as the limit of the sequence of mappings $S^n$.


here is how we mathematically write down / formalism for the mappings $S^n$.
special case where the bins are uniformly spaced along each axis, and we obtain the next iteration by subdividing each bin of the axis into two equally smaller bins.
along each axis, every bin can be located by a tuple of coordinates. a particularly elegant way to locate the square bins is to use binary notation (base 2), such that at iteration n, every coordinate has n bits of information. therefore every square can be uniquely located by a total of 2n bits.

similarly, we subdivide the unit segment into 4 every time, such that the number of segments matches the number of squares.

then

\begin{equation} \begin{aligned}
S^n &: (v_1^{(n)}, v_2^{(n)}) = (0.b_1^1 b_2^1 \cdots b_n^1, 0.b_1^2 b_2^2 \cdots b_n^2) \\
&\mapsto \alpha^{(n)} = 0.b_1 b_2 \cdots b_{2n}
\end{aligned} \end{equation}


\section{Locality of the Z-mapping and the Column mapping}


% \subsection{$S$ as the limit of a sequence of mappings}

% motivation, why do we do this

% \subsection{The Z-mapping and Column mapping}

% introduce the mappings and give plots

% \subsection{Behavior and locality measures}

% measure of locality, give examples applied to the mappings.

% see how it scales, and say what we expect

\subsection{Numerical demonstrations}

chap. 2.1, 

we stress that this toy model serves only for illustration purposes, and our results apply to any neural field

introduce coarse-graining procedure (applying the smoothness hypothesis numerically)

introduce the low-rank cycling model

\section{Proof of results}

\subsection{Analytical equivalence given $S$ measurable and bijective}
\label{sec:proof-equivalence}

\subsection{Numerical convergence}

\section{Conclusion}

\acknowledgments

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% End of Article

% \acknowledgments
% \section{Supporting Information} (optional)
% \section{Competing Interests} (optional)
% \bibliography{<name of .bib file>}

% \acknowledgments
% The Funder and award ID information you input at submission will be introduced by the publisher under a Funding Information head during production. 
% Please use this space for any additional acknowledgements and verbiage required by your funders.

% \section{Supporting Information}
% This is an optional section. Please use this space to provide information about any supporting information referred to in your manuscript.

% \section{Competing Interests}
% This is an optional section. If you declared a conflict of interest when you submitted your manuscript, please  use this space to provide details about this conflict.

% \section{Technical Terms}

% All NETN article types require Technical Terms.

% Identify approximately 10 key terms that are mentioned in your article and whose usage and definition may not be familiar across the broad readership of the journal. 
% Provide brief (20-word or less) definitions for each term, avoiding in these definitions the use of jargon, or highly technical or specialized language. 
% When the article is typeset, the Technical Terms will appear in the margins at or near their first mention in the text.

% In your manuscript, bold the first occurrence of each \textbf{Technical Term} and then provide a list of the terms and their definitions at the end of the manuscript after the references. 

%%%%%%%%%%%%%%%%%%%%%%%
%% The bibliography

%% The bibliography is made using only the entries that you cite using 
%% \cite{}, or one of the Natbib citation entries, like \citep{}, \citet{} etc.

\bibliography{references}

%ie.,
%\bibliography{bibsamp}

% \section{Technical Terms}

% \textbf{Technical Term} a key term that is mentioned in an NETN article and whose usage and definition may not be familiar across the broad readership of the journal. 

% \textbf{Technical Term} a key term that is mentioned in an NETN article and whose usage and definition may not be familiar across the broad readership of the journal. 

% \textbf{Technical Term} a key term that is mentioned in an NETN article and whose usage and definition may not be familiar across the broad readership of the journal. 

% \textbf{Technical Term} a key term that is mentioned in an NETN article and whose usage and definition may not be familiar across the broad readership of the journal. 

\end{document}

% The examples below may be helpful when you are looking
% for a quick example. They are taken from NETNsample.tex
% so you can see their results in NETNSample.pdf

% \begin{boxedtext}{box title}
% Text
% \end{boxedtext}

% or, put figure in boxed text:
% \begin{boxedtext}{box title}
% Text
% \begin{figure}
% illustration
% \caption{caption here}
% \end{figure}
% \end{boxedtext}

% or, put table in boxed text:
% \begin{boxedtext}{box title}
% Text
% \begin{table}
% \caption{caption here}
% tabular...
% \end{table}
% \end{boxedtext}

% or, put anything in whitebox within boxed text:
% \begin{boxedtext}{Showing the use of whitebox} 
% Going beyond the examination of shared topological features across..

% \whitebox{
% From $\mathcal{W}$, we can estimate the variability in the fluctuations of the functional connection between nodes $i$ and $j$ over time as:
% \begin{equation}
% s_{ij}=\sqrt{\frac{1}{T-L}\sum_{t=1}^{T-L+1}(W_{ij}(t) - m_{ij})}
% \end{equation}
% where $m_{ij}=\frac{1}{T-L+1}\sum_{t=1}^{T-L+1}W_{ij}(t)$ is the mean
% dynamic functional connectivity over time. 
% }
% ...differences across a range of network classes.
% \end{boxedtext}

% ==================
% More samples:

% \section{Sample Section}
% Text here. Text here. Text here. Text here.
% Text here. Text here. Text here. Text here.
% Text here. Text here. Text here. Text here.
% Text here. Text here. Text here. Text here.

% \subsection{Sample Subsection}
% Text here. Text here. Text here. Text here.
% Text here. Text here. Text here. Text here.
% Text here. Text here. Text here. Text here.
% Text here. Text here. Text here. Text here.

% \subsubsection{Sample Subsubsection}
% Text here. Text here. Text here. Text here.
% Text here. Text here. Text here. Text here.
% Text here. Text here. Text here. Text here.
% Text here. Text here. Text here. Text here.

% \section{Sample equations}
% \begin{equation}
% \label{eq:rhoCHT}
% \rho^{\pi}= \frac{RI + \mathbb{E}_{\pi([L,\tau_L]|\textrm{post})}
% \left[C_L(\taupav+\tau_L) \right]   +
% \displaystyle{\int_{0}^{P}}{dw~ \mathbb{E}_{\pi_{w_L}}}
% \Biggl[\/\sum_{n_{L|[\textrm{pre},w]}}C_L(\tau_L)
% \Biggr]            }      {P +
% \mathbb{E}_{\pi([L,\tau_L] |\textrm{post})}[\tau_{L}] +\taupav +
% \displaystyle{ \int_{0}^{P}}{dw~ \mathbb{E}_{\pi_{w_L}}}   
% \Biggl[\sum_{n_{L|[\textrm{pre},w]}}\tau_L\Biggr]  
% }
% \end{equation}
% As long as
% $RI - K_LP > 
% \frac{1}{\beta}$
% \begin{equation}
% %\def\theequation{5.1}
% \left.\begin{array}{lrcl}
% &\rho^{\pi} &=&  \displaystyle\frac{\beta ( RI + K_L \taupav )-1} {\beta
% (P+\taupav )}    \\[12pt]
% \hbox{and}\hbox to .25in{\hfill}&\mathbb{E}[\tau_L | \text{post}] &=&\displaystyle \frac{P+\taupav}{\beta ( RI -
% K_LP)-1}  
% \label{eq:analytical_linear}
% \end{array}\right\}\hbox to 1.25in{\hfill}
% \end{equation} 

% \subsection{Simple code sample}

% \begin{code}
% \begin{verbatim}
% procedure bubbleSort( A : list of sortable items )
%     n = length(A)
%     repeat
%        newn = 0
%        for i = 1 to n-1 inclusive do
%           if A[i-1] > A[i] then
%              swap(A[i-1], A[i])
%              newn = i
%           end if
%        end for
%        n = newn
%     until n = 0
% end procedure
% \end{verbatim}
% \end{code}

% \subsection{Algorithm environment}

% %% \begin{algorithm} takes option [p][b][t][h],  or some combination, like \begin{figure}
% %% See documentation for algorithmic.sty for more information on formatting algorithms.

% \begin{algorithm}[h]
% \caption{A sample in an algorithm environment.}
% \begin{algorithmic}
% \If {$i\geq maxval$}
%     \State $i\gets 0$
% \Else
%     \If {$i+k\leq maxval$}
%         \State $i\gets i+k$
%     \EndIf
% \EndIf
% \end{algorithmic}
% \end{algorithm}


% \section{Technical Terms}
% If you've been asked by the journal editor to provide technical terms and definitions, please bold the first occurrence of each term in the text and provide a list of the terms and their definitions at the end of the manuscript after the references. Please only supply technical terms if you've been instructed to do so by a journal editor.


% \section{Itemized Lists}

% \subsection{Roman list:}

% \begin{enumerate}
% \item[(i)] at high 
% payoffs, subjects work almost continuously, engaging in little leisure
% in between work bouts; 
% \item[(ii)] at low payoffs, they 
% engage in leisure all at once, in long bouts after working, rather
% than distributing the same amount of leisure time into multiple short
% leisure bouts; 
% \item[(iii)] subjects work continuously for the entire price duration, as long as
% the price is not very long (as shown by an analysis conducted by Y-AB, to be published separately); %(\textbf{Figure \ref{fig:task_data}D}).
% \item[(iv)] the duration of leisure bouts is variable.
% \end{enumerate}

% \subsection{Numbered list:}

% \begin{enumerate}
% \item at high 
% payoffs, subjects work almost continuously, engaging in little leisure
% inbetween work bouts; 
% \item at low payoffs, they 
% engage in leisure all at once, in long bouts after working, rather
% than distributing the same amount of leisure time into multiple short
% leisure bouts; 
% \item subjects work continuously for the entire price duration, as long as
% the price is not very long (as shown by an analysis conducted by Y-AB, to be published separately); %(\textbf{Figure \ref{fig:task_data}D}).
% \item the duration of leisure bouts is variable.
% \end{enumerate}


% \subsection{Bulleted list:}

% \begin{itemize}
% \item at high 
% payoffs, subjects work almost continuously, engaging in little leisure
% inbetween work bouts; 
% \item at low payoffs, they 
% engage in leisure all at once, in long bouts after working, rather
% than distributing the same amount of leisure time into multiple short
% leisure bouts; 
% \item subjects work continuously for the entire price duration, as long as
% the price is not very long (as shown by an analysis conducted by Y-AB, to be published separately); %(\textbf{Figure \ref{fig:task_data}D}).
% \item the duration of leisure bouts is variable.
% \end{itemize}

% \section{Natbib citation mark up}

% \subsection{Single citations}
% \noindent
% \begin{tabular}{ll}
% \bf Type&\bf Results\\
% \hline
% \verb+\citet{jon90}+&Jones et al. (1990)\\
% \verb+\citet[chap. 2]{jon90}+&Jones et al. (1990, chap. 2)\\
%     \verb+\citep{jon90}+	    &   	(Jones et al., 1990)\\
%     \verb+\citep[chap. 2]{jon90}+ 	&    	(Jones et al., 1990, chap. 2)\\
%     \verb+\citep[see][]{jon90}+ 	 &    	(see Jones et al., 1990)\\
%     \verb+\citep[see][chap. 2]{jon90}+ 	&    	(see Jones et al., 1990, chap. 2)\\
%     \verb+\citet*{jon90}+ 	    &    	Jones, Baker, and Williams (1990)\\
%     \verb+\citep*{jon90}+	    &    	(Jones, Baker, and Williams,
%     1990) \\
% \end{tabular}

% For example, here are some sample citations:
% citet:\citet{anderson}, citep:\citep{antibayes}, and
% cite*: \citet*{anderson}.

% \subsection{Multiple citations}
% Multiple citations may be made by including more than one citation
% key in the \verb+\cite+ command argument.

% \noindent
% \begin{tabular}{ll}
% \bf Type&\bf Results\\
% \hline
% \verb+\citet{jon90,jam91}+&Jones et al. (1990); James et al. (1991)\\
% \verb+\citep{jon90,jam91}+&(Jones et al., 1990; James et al. 1991)\\
% \verb+\citep{jon90,jon91}+&(Jones et al., 1990, 1991)\\
% \verb+\citep{jon90a,jon90b}+&(Jones et al., 1990a,b)\\
% \end{tabular}

% For example, multiple citations from the CompPsychSample bibliography:
% citet:\citet{anderson,antibayes}, citep:\citep{anderson,antibayes}.
% As you see, the citations are automatically hyperlinked to their
% reference in the bibliography.

% \newpage

% \section{Sample figures}

% \begin{figure}[h] 
% \centerline{\includegraphics[width=\textwidth]{Fig1}}
% \caption{(Colour online) \textbf{Task and key features of the
%  data.} \\
%  A) Cumulative handling time (CHT) task. Grey bars denote work
% (depressing a lever), white gaps show leisure. The subject must
%  accumulate work up to a total period of time called the
% \emph{price} ($P$) in order to obtain a single reward (black dot) of subjective reward
% intensity $RI$. The trial duration is $25\times \mathrm{price}$ (plus
% $2$s each time the price is attained, during which the lever is retracted so it cannot
% work; not shown).
% }
% \label{fig:task_data}
% \end{figure}

% \begin{figure}[ht] 
% \widefigure{\fullpagewidth}{Fig1}
% \caption{(Colour online) \textbf{Task and key features of the
%  data.} \\
% A) Cumulative handling time (CHT) task. Grey bars denote work
% (depressing a lever), white gaps show leisure. The subject must
% accumulate work up to a total period of time called the
% \emph{price} ($P$) in order to obtain a single reward (black dot) of subjective reward
% intensity $RI$. The trial duration is $25\times \mathrm{price}$ (plus
% $2$s each time the price is attained, during which the lever is retracted so it cannot
% work; not shown).
% }
% \label{newfig:task_data}
% \end{figure}

% \clearpage
% \section{Sample tables}

% \begin{table}[!ht]
% \caption{Time of the Transition Between Phase 1 and Phase 2$^{a}$}
% \label{tab:label}
% \centering
% \begin{tabular}{lc}
% \hline
%  Run  & Time (min)  \\
% \hline
%   $l1$  & 260   \\
%   $l2$  & 300   \\
%   $l3$  & 340   \\
%   $h1$  & 270   \\
%   $h2$  & 250   \\
%   $h3$  & 380   \\
%   $r1$  & 370   \\
%   $r2$  & 390   \\
% \hline
% \multicolumn{2}{l}{$^{a}$Table note text here.}
% \end{tabular}
% \end{table}

% \begin{table}[ht]
% \widecaption{Sample table taken from [treu03]\label{tbl-1}}
% \begin{widetable}
% \advance\tabcolsep-1pt
% \small
% \begin{tabular}{ccrrccccccccc}
% \hline
% \bf 
% POS &\bf  chip &\multicolumn1c{\bf ID} &\multicolumn1c{\bf X}
% &\multicolumn1c{\bf Y} &\bf
% RA &\bf DEC &\bf IAU$\pm$ $\delta$ IAU &\bf
% IAP1$\pm$ $\delta$ IAP1 &\bf IAP2 $\pm$ $\delta$
% IAP2 &\bf star &\bf E &\bf Comment\\
% \hline
% 0 & 2 & 1 & 1370.99 & 57.35\rlap{$^a$}    &   6.651120 &  17.131149 &
% 21.344$\pm$0.006\rlap{$^b$}  & 2 4.385$\pm$0.016 & 23.528$\pm$0.013 & 0.0 & 9 & -    \\
% 0 & 2 & 2 & 1476.62 & 8.03     &   6.651480 &  17.129572 & 21.641$\pm$0.005  & 2 3.141$\pm$0.007 & 22.007$\pm$0.004 & 0.0 & 9 & -    \\
% 0 & 2 & 3 & 1079.62 & 28.92    &   6.652430 &  17.135000 & 23.953$\pm$0.030  & 2 4.890$\pm$0.023 & 24.240$\pm$0.023 & 0.0 & - & -    \\
% 0 & 2 & 4 & 114.58  & 21.22    &   6.655560 &  17.148020 & 23.801$\pm$0.025  & 2 5.039$\pm$0.026 & 24.112$\pm$0.021 & 0.0 & - & -    \\
% 0 & 2 & 5 & 46.78   & 19.46    &   6.655800 &  17.148932 & 23.012$\pm$0.012  & 2 3.924$\pm$0.012 & 23.282$\pm$0.011 & 0.0 & - & -    \\
% 0 & 2 & 6 & 1441.84 & 16.16    &   6.651480 &  17.130072 & 24.393$\pm$0.045  & 2 6.099$\pm$0.062 & 25.119$\pm$0.049 & 0.0 & - & -    \\
% 0 & 2 & 7 & 205.43  & 3.96     &   6.655520 &  17.146742 & 24.424$\pm$0.032  & 2 5.028$\pm$0.025 & 24.597$\pm$0.027 & 0.0 & - & -    \\
% 0 & 2 & 8 & 1321.63 & 9.76     &   6.651950 &  17.131672 &
% 22.189$\pm$0.011  & 2 4.743$\pm$0.021 & 23.298$\pm$0.011 & 0.0 & 4 &
% edge \\
% \hline
% \multicolumn{13}{l}{%
% Table 2 is published in its entirety in the electronic
% edition of the {\it Astrophysical Journal}.}\\[3pt]
% \multicolumn{13}{l}{%
% $^a$ Sample footnote for table 2.}\\[3pt]
% \multicolumn{13}{l}{%
% $^b$ Another sample footnote for table 2.}
% \end{tabular}
% \end{widetable}
% \end{table}

% \begin{table}[p]
% \rotatebox{90}{\vbox{\hsize=\textheight
% \caption{Here is a caption for a table that is found in landscape
% mode.}
% \begin{tabular}{ccrrccccccccc}
% \hline
% \bf 
% POS &\bf  chip &\multicolumn1c{\bf ID} &\multicolumn1c{\bf X}
% &\multicolumn1c{\bf Y} &\bf
% RA &\bf DEC &\bf IAU$\pm$ $\delta$ IAU &\bf
% IAP1$\pm$ $\delta$ IAP1 &\bf IAP2 $\pm$ $\delta$
% IAP2 &\bf star &\bf E &\bf Comment\\
% \hline
% 0 & 2 & 1 & 1370.99 & 57.35\rlap{$^a$}    &   6.651120 &  17.131149 &
% 21.344$\pm$0.006\rlap{$^b$}  & 2 4.385$\pm$0.016 & 23.528$\pm$0.013 & 0.0 & 9 & -    \\
% 0 & 2 & 2 & 1476.62 & 8.03     &   6.651480 &  17.129572 & 21.641$\pm$0.005  & 2 3.141$\pm$0.007 & 22.007$\pm$0.004 & 0.0 & 9 & -    \\
% 0 & 2 & 3 & 1079.62 & 28.92    &   6.652430 &  17.135000 & 23.953$\pm$0.030  & 2 4.890$\pm$0.023 & 24.240$\pm$0.023 & 0.0 & - & -    \\
% 0 & 2 & 4 & 114.58  & 21.22    &   6.655560 &  17.148020 & 23.801$\pm$0.025  & 2 5.039$\pm$0.026 & 24.112$\pm$0.021 & 0.0 & - & -    \\
% 0 & 2 & 5 & 46.78   & 19.46    &   6.655800 &  17.148932 & 23.012$\pm$0.012  & 2 3.924$\pm$0.012 & 23.282$\pm$0.011 & 0.0 & - & -    \\
% 0 & 2 & 6 & 1441.84 & 16.16    &   6.651480 &  17.130072 & 24.393$\pm$0.045  & 2 6.099$\pm$0.062 & 25.119$\pm$0.049 & 0.0 & - & -    \\
% 0 & 2 & 7 & 205.43  & 3.96     &   6.655520 &  17.146742 & 24.424$\pm$0.032  & 2 5.028$\pm$0.025 & 24.597$\pm$0.027 & 0.0 & - & -    \\
% 0 & 2 & 8 & 1321.63 & 9.76     &   6.651950 &  17.131672 &
% 22.189$\pm$0.011  & 2 4.743$\pm$0.021 & 23.298$\pm$0.011 & 0.0 & 4 &
% edge \\
% \hline
% \multicolumn{13}{l}{%
% Table 2 is published in its entirety in the electronic
% edition of the {\it Astrophysical Journal}.}\\[3pt]
% \multicolumn{13}{l}{%
% $^a$ Sample footnote for table 2.}\\[3pt]
% \multicolumn{13}{l}{%
% $^b$ Another sample footnote for table 2.}
% \end{tabular}
% }}
% \end{table}
% \clearpage


% \vglue 3in
% Example of table continuing over pages:


% \begin{center}
% \begin{longtable}{ccc@{}}
% \caption{ApJ costs from 1991 to 2013
% \label{tab:table}} \\[2pt]
% \hline
% \bf Year & \bf Subscription & \bf Publication \\
%  & \bf cost &\bf charges\\
%  & \bf(\$) & \bf (\$/page)\\
% \hline
% \endfirsthead

% \multicolumn3c{Table \thetable, \it continued from previous page.}\\[6pt]
% \multicolumn3c{ApJ costs from 1991 to 2013}\\[2pt]
% \hline
% \bf Year & \bf Subscription & \bf Publication \\
%  & \bf cost &\bf charges\\
%  & \bf(\$) & \bf (\$/page)\\
% \hline
% \endhead
% \\\hline
% \\[-8pt]
% \multicolumn{3}{r}{\it Table continued on next page}\\ 
% \endfoot

% \hline
% \endlastfoot

% 1991 & 600 & 100 \\
% 1992 & 650 & 105 \\
% 1993 & 550 & 103 \\
% 1994 & 450 & 110 \\
% 1995 & 410 & 112 \\
% 1996 & 400 & 114 \\
% 1997 & 525 & 115 \\
% 1998 & 590 & 116 \\
% 1999 & 575 & 115 \\
% 2000 & 450 & 103 \\
% 2001 & 490 &  90 \\
% 2002 & 500 &  88 \\
% 2003 & 450 &  90 \\
% 2004 & 460 &  88 \\
% 2005 & 440 &  79 \\
% 2006 & 350 &  77 \\
% 2007 & 325 &  70 \\
% 2008 & 320 &  65 \\
% 2009 & 190 &  68 \\
% 2010 & 280 &  70 \\
% 2011 & 275 &  68 \\
% 2012 & 150 &  56 \\
% 2013 & 140 &  55 \\
% \end{longtable}
% \end{center}

% %%%%%%%%%%%%%%%%%%%%%%%
% %% The bibliography

% %% The bibliography is made using only
% %% the entries that you cite using \cite{}, or one of the Natbib citation
% %% entries, like \citep{}, \citet{} etc.

% \bibliography{bibsamp}

% NO APPENDICES allowed in the Network Neuroscience Style.

% Please submit Supporting Information documents as PDFs in a format ready to publish.
